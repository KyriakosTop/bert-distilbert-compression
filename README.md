# DistilBERT Compression for Efficient Inference

This repository contains the experiments and findings of my MSc project on compressing transformer models for low-resource hardware.

The focus is on applying quantization and pruning techniques to a fine-tuned DistilBERT model on the SST-2 sentiment classification task. The project evaluates trade-offs between accuracy, inference time, and memory usage across different devices (CPU, GTX 1050 Ti, and T4 GPU).
