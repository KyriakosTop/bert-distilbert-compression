{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtTPFy0q4d17OIvw1DtK4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "466048a4f81f4f69a38a0d3283a51434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b67cb27de716499db04c51af21549957",
              "IPY_MODEL_a041dd5c86ff4844870747a25c2fd187",
              "IPY_MODEL_bb64593642344f14a8a56c1212f9b2be"
            ],
            "layout": "IPY_MODEL_5c280305cb784952940fb1e16f11dba7"
          }
        },
        "b67cb27de716499db04c51af21549957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f6df807e4da4029a0ea9e1b3aa077bb",
            "placeholder": "​",
            "style": "IPY_MODEL_d5c8426ec84143d586c6b0e58503a9b0",
            "value": "Map: 100%"
          }
        },
        "a041dd5c86ff4844870747a25c2fd187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98f3b489b4ba4541b36747eb204640a1",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc59809bd9da4894936aa9c7bc04d35a",
            "value": 100
          }
        },
        "bb64593642344f14a8a56c1212f9b2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cddd890f9d0040db864a98cc84cbf567",
            "placeholder": "​",
            "style": "IPY_MODEL_176a2b1a7c8348269378cfde3612270a",
            "value": " 100/100 [00:00&lt;00:00, 1691.72 examples/s]"
          }
        },
        "5c280305cb784952940fb1e16f11dba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f6df807e4da4029a0ea9e1b3aa077bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c8426ec84143d586c6b0e58503a9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98f3b489b4ba4541b36747eb204640a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc59809bd9da4894936aa9c7bc04d35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cddd890f9d0040db864a98cc84cbf567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "176a2b1a7c8348269378cfde3612270a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyriakosTop/distilbert-compression/blob/main/bert_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-base Quantization on GPU – FP32, 8-bit, and 4-bit Inference\n",
        "\n",
        "This notebook benchmarks the inference performance of the BERT-base model fine-tuned on the SST-2 sentiment classification task using a T4 GPU in Google Colab.\n",
        "\n",
        "We evaluate:\n",
        "\n",
        "- Full-precision (FP32) inference\n",
        "- 8-bit quantized inference using bitsandbytes\n",
        "- 4-bit quantized inference using bitsandbytes\n",
        "\n",
        "Each configuration is tested on 100 validation samples from SST-2.  \n",
        "We measure:\n",
        "\n",
        "- Accuracy\n",
        "- Average latency per sample (in milliseconds)\n",
        "- System RAM usage (MB)\n",
        "- GPU VRAM usage (total and delta in MB)\n",
        "\n",
        "This notebook is designed to assess whether quantization is more effective for larger models like BERT-base, compared to smaller architectures such as DistilBERT."
      ],
      "metadata": {
        "id": "DIaWcEMsq9q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets evaluate bitsandbytes accelerate pynvml psutil\n",
        "\n",
        "# Re-imports if kernel was reset or skipped\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import psutil\n",
        "import pynvml\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7EjkUU4-q91R",
        "outputId": "7f1b4227-ae11-495e-ba9e-238ff9147e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load BERT-base and SST-2 Validation Samples\n",
        "\n",
        "We use the `bert-base-uncased` model from Hugging Face, which has 12 transformer layers and ~110 million parameters.  \n",
        "To maintain consistency with earlier experiments, we evaluate on the first 100 samples from the SST-2 validation set.  \n",
        "Each sample is tokenized to a fixed length of 128 tokens, and wrapped in a PyTorch-compatible dataset class."
      ],
      "metadata": {
        "id": "69NZOB4_-oxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model ID and tokenizer\n",
        "model_id = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load SST-2 validation set (first 100 samples)\n",
        "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:100]\")\n",
        "\n",
        "# Tokenize with max length 128\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function)\n",
        "\n",
        "# Wrap into PyTorch-style dataset\n",
        "class SST2Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.input_ids = [torch.tensor(x) for x in hf_dataset[\"input_ids\"]]\n",
        "        self.attention_mask = [torch.tensor(x) for x in hf_dataset[\"attention_mask\"]]\n",
        "        self.labels = [torch.tensor(x) for x in hf_dataset[\"label\"]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx].to(\"cuda\"),\n",
        "            \"attention_mask\": self.attention_mask[idx].to(\"cuda\"),\n",
        "            \"label\": self.labels[idx].item()\n",
        "        }\n",
        "\n",
        "dataset = SST2Dataset(tokenized_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "466048a4f81f4f69a38a0d3283a51434",
            "b67cb27de716499db04c51af21549957",
            "a041dd5c86ff4844870747a25c2fd187",
            "bb64593642344f14a8a56c1212f9b2be",
            "5c280305cb784952940fb1e16f11dba7",
            "2f6df807e4da4029a0ea9e1b3aa077bb",
            "d5c8426ec84143d586c6b0e58503a9b0",
            "98f3b489b4ba4541b36747eb204640a1",
            "dc59809bd9da4894936aa9c7bc04d35a",
            "cddd890f9d0040db864a98cc84cbf567",
            "176a2b1a7c8348269378cfde3612270a"
          ]
        },
        "collapsed": true,
        "id": "iUt__dGP-qMO",
        "outputId": "31924249-80b7-4717-e2ce-b060b827e9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "466048a4f81f4f69a38a0d3283a51434"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Evaluation Function\n",
        "\n",
        "This function evaluates the model on GPU using the 100-tokenized SST-2 samples.  \n",
        "It measures:\n",
        "\n",
        "- Accuracy\n",
        "- Average latency per sample (in seconds)\n",
        "- System RAM usage increase (MB)\n",
        "- GPU VRAM usage increase during inference (MB)\n",
        "- Total VRAM usage after model load (MB)\n",
        "\n",
        "The function assumes the model is already loaded onto the GPU and runs inference sample-by-sample without batching, to allow fine-grained latency measurement."
      ],
      "metadata": {
        "id": "9CryrMr6JQEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset):\n",
        "    model.eval()\n",
        "    process = psutil.Process(os.getpid())\n",
        "\n",
        "    # Initialize GPU memory tracker\n",
        "    pynvml.nvmlInit()\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "    model_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)  # MB\n",
        "    start_ram = process.memory_info().rss\n",
        "    start_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n",
        "\n",
        "    correct = 0\n",
        "    latencies = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in dataset:\n",
        "            inputs = {\n",
        "                \"input_ids\": sample[\"input_ids\"].unsqueeze(0),\n",
        "                \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0),\n",
        "            }\n",
        "            label = sample[\"label\"]\n",
        "\n",
        "            start_time = time.time()\n",
        "            outputs = model(**inputs)\n",
        "            end_time = time.time()\n",
        "\n",
        "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
        "            correct += (pred == label)\n",
        "            latencies.append(end_time - start_time)\n",
        "\n",
        "    end_ram = process.memory_info().rss\n",
        "    end_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n",
        "    pynvml.nvmlShutdown()\n",
        "\n",
        "    delta_ram = (end_ram - start_ram) / (1024 ** 2)\n",
        "    delta_vram = (end_vram - start_vram) / (1024 ** 2)\n",
        "    avg_latency = np.mean(latencies)\n",
        "    accuracy = correct / len(dataset)\n",
        "\n",
        "    return accuracy, avg_latency, delta_ram, delta_vram, model_vram"
      ],
      "metadata": {
        "id": "TAq4utxUJY_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Run Full-Precision (FP32) Inference\n",
        "\n",
        "We load the full-precision BERT-base model fine-tuned on SST-2 and run inference on the 100 validation samples using a T4 GPU.\n",
        "\n",
        "This serves as our performance baseline for comparison with quantized versions."
      ],
      "metadata": {
        "id": "PytEzceJIhrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory before loading\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load BERT-base FP32 model to GPU\n",
        "model_fp32 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"textattack/bert-base-uncased-SST-2\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Evaluate\n",
        "accuracy_fp32, latency_fp32, ram_fp32, vram_delta_fp32, vram_model_fp32 = evaluate_model(model_fp32, dataset)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy (FP32): {accuracy_fp32:.2%}\")\n",
        "print(f\"Latency per sample (FP32): {latency_fp32 * 1000:.2f} ms\")\n",
        "print(f\"System RAM usage increase (FP32): {ram_fp32:.2f} MB\")\n",
        "print(f\"GPU VRAM usage increase during inference (FP32): {vram_delta_fp32:.2f} MB\")\n",
        "print(f\"Total VRAM after model load (FP32): {vram_model_fp32:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWkVe3o5IjI-",
        "outputId": "e96d5d50-c2e3-4cb5-9c3b-f65205a029c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (FP32): 92.00%\n",
            "Latency per sample (FP32): 12.83 ms\n",
            "System RAM usage increase (FP32): 63.26 MB\n",
            "GPU VRAM usage increase during inference (FP32): 32.00 MB\n",
            "Total VRAM after model load (FP32): 1289.88 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run 8-bit Quantized Inference (bitsandbytes)\n",
        "\n",
        "We now evaluate the 8-bit quantized version of BERT-base using the `bitsandbytes` backend.  \n",
        "All linear layers are quantized to 8-bit precision during model loading.  \n",
        "This test will help us assess whether quantization improves latency or memory usage on a larger model like BERT."
      ],
      "metadata": {
        "id": "ORGADuHkKvaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory before loading 8-bit model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Configure 8-bit quantization\n",
        "bnb_config_8bit = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    device_map={\"\": 0}  # Force entire model to GPU\n",
        ")\n",
        "\n",
        "# Load 8-bit quantized model\n",
        "model_int8 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"textattack/bert-base-uncased-SST-2\",\n",
        "    quantization_config=bnb_config_8bit\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "accuracy_int8, latency_int8, ram_int8, vram_delta_int8, vram_model_int8 = evaluate_model(model_int8, dataset)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy (8-bit): {accuracy_int8:.2%}\")\n",
        "print(f\"Latency per sample (8-bit): {latency_int8 * 1000:.2f} ms\")\n",
        "print(f\"System RAM usage increase (8-bit): {ram_int8:.2f} MB\")\n",
        "print(f\"GPU VRAM usage increase during inference (8-bit): {vram_delta_int8:.2f} MB\")\n",
        "print(f\"Total VRAM after model load (8-bit): {vram_model_int8:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TF69D77KxY5",
        "outputId": "3ad9fb5c-b7f5-4794-be9c-14b46e1a2f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (8-bit): 92.00%\n",
            "Latency per sample (8-bit): 94.04 ms\n",
            "System RAM usage increase (8-bit): 20.96 MB\n",
            "GPU VRAM usage increase during inference (8-bit): 12.00 MB\n",
            "Total VRAM after model load (8-bit): 1081.88 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run 4-bit Quantized Inference (bitsandbytes)\n",
        "\n",
        "We now evaluate the 4-bit quantized version of BERT-base using bitsandbytes.  \n",
        "This uses QLoRA-style quantization, which compresses linear layers to 4-bit precision using custom CUDA kernels.  \n",
        "This experiment helps assess whether 4-bit quantization can reduce memory further while maintaining accuracy and acceptable latency."
      ],
      "metadata": {
        "id": "K3X0RBM-O5-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory before loading 4-bit model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "\n",
        "# Load 4-bit quantized model\n",
        "model_4bit = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"textattack/bert-base-uncased-SST-2\",\n",
        "    quantization_config=bnb_config_4bit\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "accuracy_4bit, latency_4bit, ram_4bit, vram_delta_4bit, vram_model_4bit = evaluate_model(model_4bit, dataset)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy (4-bit): {accuracy_4bit:.2%}\")\n",
        "print(f\"Latency per sample (4-bit): {latency_4bit * 1000:.2f} ms\")\n",
        "print(f\"System RAM usage increase (4-bit): {ram_4bit:.2f} MB\")\n",
        "print(f\"GPU VRAM usage increase during inference (4-bit): {vram_delta_4bit:.2f} MB\")\n",
        "print(f\"Total VRAM after model load (4-bit): {vram_model_4bit:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ9V4IyjPF8S",
        "outputId": "cd883845-d61a-4b05-bd68-df7239f07482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (4-bit): 92.00%\n",
            "Latency per sample (4-bit): 20.75 ms\n",
            "System RAM usage increase (4-bit): 0.51 MB\n",
            "GPU VRAM usage increase during inference (4-bit): 6.00 MB\n",
            "Total VRAM after model load (4-bit): 1195.88 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Summary of Results\n",
        "\n",
        "The following table summarizes the performance of BERT-base across FP32, 8-bit, and 4-bit configurations, evaluated on 100 SST-2 validation samples using a T4 GPU.\n",
        "\n",
        "| Model   | Accuracy | Latency (ms) | RAM ↑ (MB) | VRAM ↑ (MB) | Total VRAM (MB) |\n",
        "|---------|----------|---------------|-------------|--------------|------------------|\n",
        "| FP32    | 92.00%   | 12.83         | 63.26       | 32.00        | 1289.88          |\n",
        "| 8-bit   | 92.00%   | 94.04         | 20.96       | 12.00        | 1081.88          |\n",
        "| 4-bit   | 92.00%   | 20.75         | 0.51        | 6.00         | 1195.88          |"
      ],
      "metadata": {
        "id": "Isqs32r6Qm_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Observations\n",
        "\n",
        "- All three model variants achieved identical accuracy (92.00%) on the 100-sample SST-2 validation set, suggesting that post-training quantization does not harm prediction quality on this task.\n",
        "- The 8-bit model incurred a significant latency increase (94 ms per sample), likely due to bitsandbytes kernel overhead, consistent with previous experiments on smaller models.\n",
        "- The 4-bit quantized model offered the best memory efficiency and acceptable latency (20.75 ms), making it the most balanced choice for low-memory scenarios.\n",
        "- These results suggest that quantization benefits scale better with larger models like BERT-base, especially when using 4-bit fused kernels on modern GPUs."
      ],
      "metadata": {
        "id": "Cgxu8FEXSV7G"
      }
    }
  ]
}