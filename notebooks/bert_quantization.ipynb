{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIaWcEMsq9q1"
   },
   "source": [
    "# BERT-base Quantization on GPU â€“ FP32, 8-bit, and 4-bit Inference\n",
    "\n",
    "This notebook benchmarks the inference performance of the BERT-base model fine-tuned on the SST-2 sentiment classification task using a T4 GPU in Google Colab.\n",
    "\n",
    "We evaluate:\n",
    "\n",
    "- Full-precision (FP32) inference\n",
    "- 8-bit quantized inference using bitsandbytes\n",
    "- 4-bit quantized inference using bitsandbytes\n",
    "\n",
    "Each configuration is tested on 100 validation samples from SST-2.  \n",
    "We measure:\n",
    "\n",
    "- Accuracy\n",
    "- Average latency per sample (in milliseconds)\n",
    "- System RAM usage (MB)\n",
    "- GPU VRAM usage (total and delta in MB)\n",
    "\n",
    "This notebook is designed to assess whether quantization is more effective for larger models like BERT-base, compared to smaller architectures such as DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 147262,
     "status": "ok",
     "timestamp": 1753808226332,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "7EjkUU4-q91R",
    "outputId": "7f1b4227-ae11-495e-ba9e-238ff9147e2a"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets evaluate bitsandbytes accelerate pynvml psutil\n",
    "\n",
    "# Re-imports if kernel was reset or skipped\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import pynvml\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69NZOB4_-oxx"
   },
   "source": [
    "## 1. Load BERT-base and SST-2 Validation Samples\n",
    "\n",
    "We use the `bert-base-uncased` model from Hugging Face, which has 12 transformer layers and ~110 million parameters.  \n",
    "To maintain consistency with earlier experiments, we evaluate on the first 100 samples from the SST-2 validation set.  \n",
    "Each sample is tokenized to a fixed length of 128 tokens, and wrapped in a PyTorch-compatible dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "466048a4f81f4f69a38a0d3283a51434",
      "b67cb27de716499db04c51af21549957",
      "a041dd5c86ff4844870747a25c2fd187",
      "bb64593642344f14a8a56c1212f9b2be",
      "5c280305cb784952940fb1e16f11dba7",
      "2f6df807e4da4029a0ea9e1b3aa077bb",
      "d5c8426ec84143d586c6b0e58503a9b0",
      "98f3b489b4ba4541b36747eb204640a1",
      "dc59809bd9da4894936aa9c7bc04d35a",
      "cddd890f9d0040db864a98cc84cbf567",
      "176a2b1a7c8348269378cfde3612270a"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6191,
     "status": "ok",
     "timestamp": 1753808757504,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "iUt__dGP-qMO",
    "outputId": "31924249-80b7-4717-e2ce-b060b827e9d6"
   },
   "outputs": [],
   "source": [
    "# Define model ID and tokenizer\n",
    "model_id = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load SST-2 validation set (first 100 samples)\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:100]\")\n",
    "\n",
    "# Tokenize with max length 128\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function)\n",
    "\n",
    "# Wrap into PyTorch-style dataset\n",
    "class SST2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.input_ids = [torch.tensor(x) for x in hf_dataset[\"input_ids\"]]\n",
    "        self.attention_mask = [torch.tensor(x) for x in hf_dataset[\"attention_mask\"]]\n",
    "        self.labels = [torch.tensor(x) for x in hf_dataset[\"label\"]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx].to(\"cuda\"),\n",
    "            \"attention_mask\": self.attention_mask[idx].to(\"cuda\"),\n",
    "            \"label\": self.labels[idx].item()\n",
    "        }\n",
    "\n",
    "dataset = SST2Dataset(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CryrMr6JQEL"
   },
   "source": [
    "## 2. Define Evaluation Function\n",
    "\n",
    "This function evaluates the model on GPU using the 100-tokenized SST-2 samples.  \n",
    "It measures:\n",
    "\n",
    "- Accuracy\n",
    "- Average latency per sample (in seconds)\n",
    "- System RAM usage increase (MB)\n",
    "- GPU VRAM usage increase during inference (MB)\n",
    "- Total VRAM usage after model load (MB)\n",
    "\n",
    "The function assumes the model is already loaded onto the GPU and runs inference sample-by-sample without batching, to allow fine-grained latency measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1753808759959,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "TAq4utxUJY_q"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    # Initialize GPU memory tracker\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "    model_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)  # MB\n",
    "    start_ram = process.memory_info().rss\n",
    "    start_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n",
    "\n",
    "    correct = 0\n",
    "    latencies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            inputs = {\n",
    "                \"input_ids\": sample[\"input_ids\"].unsqueeze(0),\n",
    "                \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0),\n",
    "            }\n",
    "            label = sample[\"label\"]\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "            correct += (pred == label)\n",
    "            latencies.append(end_time - start_time)\n",
    "\n",
    "    end_ram = process.memory_info().rss\n",
    "    end_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    delta_ram = (end_ram - start_ram) / (1024 ** 2)\n",
    "    delta_vram = (end_vram - start_vram) / (1024 ** 2)\n",
    "    avg_latency = np.mean(latencies)\n",
    "    accuracy = correct / len(dataset)\n",
    "\n",
    "    return accuracy, avg_latency, delta_ram, delta_vram, model_vram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PytEzceJIhrq"
   },
   "source": [
    "## 3. Run Full-Precision (FP32) Inference\n",
    "\n",
    "We load the full-precision BERT-base model fine-tuned on SST-2 and run inference on the 100 validation samples using a T4 GPU.\n",
    "\n",
    "This serves as our performance baseline for comparison with quantized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2546,
     "status": "ok",
     "timestamp": 1753808769152,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "sWkVe3o5IjI-",
    "outputId": "e96d5d50-c2e3-4cb5-9c3b-f65205a029c5"
   },
   "outputs": [],
   "source": [
    "# Clear GPU memory before loading\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load BERT-base FP32 model to GPU\n",
    "model_fp32 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-SST-2\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Evaluate\n",
    "accuracy_fp32, latency_fp32, ram_fp32, vram_delta_fp32, vram_model_fp32 = evaluate_model(model_fp32, dataset)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy (FP32): {accuracy_fp32:.2%}\")\n",
    "print(f\"Latency per sample (FP32): {latency_fp32 * 1000:.2f} ms\")\n",
    "print(f\"System RAM usage increase (FP32): {ram_fp32:.2f} MB\")\n",
    "print(f\"GPU VRAM usage increase during inference (FP32): {vram_delta_fp32:.2f} MB\")\n",
    "print(f\"Total VRAM after model load (FP32): {vram_model_fp32:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORGADuHkKvaP"
   },
   "source": [
    "## 4. Run 8-bit Quantized Inference (bitsandbytes)\n",
    "\n",
    "We now evaluate the 8-bit quantized version of BERT-base using the `bitsandbytes` backend.  \n",
    "All linear layers are quantized to 8-bit precision during model loading.  \n",
    "This test will help us assess whether quantization improves latency or memory usage on a larger model like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11567,
     "status": "ok",
     "timestamp": 1753809114185,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "-TF69D77KxY5",
    "outputId": "3ad9fb5c-b7f5-4794-be9c-14b46e1a2f20"
   },
   "outputs": [],
   "source": [
    "# Clear GPU memory before loading 8-bit model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\": 0}  # Force entire model to GPU\n",
    ")\n",
    "\n",
    "# Load 8-bit quantized model\n",
    "model_int8 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-SST-2\",\n",
    "    quantization_config=bnb_config_8bit\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "accuracy_int8, latency_int8, ram_int8, vram_delta_int8, vram_model_int8 = evaluate_model(model_int8, dataset)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy (8-bit): {accuracy_int8:.2%}\")\n",
    "print(f\"Latency per sample (8-bit): {latency_int8 * 1000:.2f} ms\")\n",
    "print(f\"System RAM usage increase (8-bit): {ram_int8:.2f} MB\")\n",
    "print(f\"GPU VRAM usage increase during inference (8-bit): {vram_delta_int8:.2f} MB\")\n",
    "print(f\"Total VRAM after model load (8-bit): {vram_model_int8:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3X0RBM-O5-6"
   },
   "source": [
    "## 5. Run 4-bit Quantized Inference (bitsandbytes)\n",
    "\n",
    "We now evaluate the 4-bit quantized version of BERT-base using bitsandbytes.  \n",
    "This uses QLoRA-style quantization, which compresses linear layers to 4-bit precision using custom CUDA kernels.  \n",
    "This experiment helps assess whether 4-bit quantization can reduce memory further while maintaining accuracy and acceptable latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3435,
     "status": "ok",
     "timestamp": 1753810262728,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "uJ9V4IyjPF8S",
    "outputId": "cd883845-d61a-4b05-bd68-df7239f07482"
   },
   "outputs": [],
   "source": [
    "# Clear GPU memory before loading 4-bit model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "model_4bit = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"textattack/bert-base-uncased-SST-2\",\n",
    "    quantization_config=bnb_config_4bit\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "accuracy_4bit, latency_4bit, ram_4bit, vram_delta_4bit, vram_model_4bit = evaluate_model(model_4bit, dataset)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy (4-bit): {accuracy_4bit:.2%}\")\n",
    "print(f\"Latency per sample (4-bit): {latency_4bit * 1000:.2f} ms\")\n",
    "print(f\"System RAM usage increase (4-bit): {ram_4bit:.2f} MB\")\n",
    "print(f\"GPU VRAM usage increase during inference (4-bit): {vram_delta_4bit:.2f} MB\")\n",
    "print(f\"Total VRAM after model load (4-bit): {vram_model_4bit:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Isqs32r6Qm_3"
   },
   "source": [
    "## 6. Summary of Results\n",
    "\n",
    "The following table summarizes the performance of BERT-base across FP32, 8-bit, and 4-bit configurations, evaluated on 100 SST-2 validation samples using a T4 GPU.\n",
    "\n",
    "| Model   | Accuracy | Latency (ms) | RAM â†‘ (MB) | VRAM â†‘ (MB) | Total VRAM (MB) |\n",
    "|---------|----------|---------------|-------------|--------------|------------------|\n",
    "| FP32    | 92.00%   | 12.83         | 63.26       | 32.00        | 1289.88          |\n",
    "| 8-bit   | 92.00%   | 94.04         | 20.96       | 12.00        | 1081.88          |\n",
    "| 4-bit   | 92.00%   | 20.75         | 0.51        | 6.00         | 1195.88          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgxu8FEXSV7G"
   },
   "source": [
    "## 7. Observations\n",
    "\n",
    "- All three model variants achieved identical accuracy (92.00%) on the 100-sample SST-2 validation set, suggesting that post-training quantization does not harm prediction quality on this task.\n",
    "- The 8-bit model incurred a significant latency increase (94 ms per sample), likely due to bitsandbytes kernel overhead, consistent with previous experiments on smaller models.\n",
    "- The 4-bit quantized model offered the best memory efficiency and acceptable latency (20.75 ms), making it the most balanced choice for low-memory scenarios.\n",
    "- These results suggest that quantization benefits scale better with larger models like BERT-base, especially when using 4-bit fused kernels on modern GPUs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOtTPFy0q4d17OIvw1DtK4s",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "176a2b1a7c8348269378cfde3612270a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f6df807e4da4029a0ea9e1b3aa077bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "466048a4f81f4f69a38a0d3283a51434": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b67cb27de716499db04c51af21549957",
       "IPY_MODEL_a041dd5c86ff4844870747a25c2fd187",
       "IPY_MODEL_bb64593642344f14a8a56c1212f9b2be"
      ],
      "layout": "IPY_MODEL_5c280305cb784952940fb1e16f11dba7"
     }
    },
    "5c280305cb784952940fb1e16f11dba7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98f3b489b4ba4541b36747eb204640a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a041dd5c86ff4844870747a25c2fd187": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98f3b489b4ba4541b36747eb204640a1",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc59809bd9da4894936aa9c7bc04d35a",
      "value": 100
     }
    },
    "b67cb27de716499db04c51af21549957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f6df807e4da4029a0ea9e1b3aa077bb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d5c8426ec84143d586c6b0e58503a9b0",
      "value": "Map:â€‡100%"
     }
    },
    "bb64593642344f14a8a56c1212f9b2be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cddd890f9d0040db864a98cc84cbf567",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_176a2b1a7c8348269378cfde3612270a",
      "value": "â€‡100/100â€‡[00:00&lt;00:00,â€‡1691.72â€‡examples/s]"
     }
    },
    "cddd890f9d0040db864a98cc84cbf567": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5c8426ec84143d586c6b0e58503a9b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc59809bd9da4894936aa9c7bc04d35a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
