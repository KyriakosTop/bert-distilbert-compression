{"cells":[{"cell_type":"markdown","metadata":{"id":"_tGmPV3ij_tU"},"source":["# DistilBERT Compression Sweep (CPU) â€“ Pruning + Quantization\n","\n","This notebook evaluates the performance of a fine-tuned DistilBERT model on the SST-2 sentiment classification task under structured pruning and 8-bit dynamic quantization.\n","\n","We compare:\n","- Pruned FP32 inference\n","- Pruned + Quantized (INT8) inference\n","\n","Across pruning levels:\n","- 30%\n","- 40%\n","- 50%\n","\n","Reported metrics:\n","- Accuracy\n","- Latency (ms/sample)\n","- Total RAM usage (MB)\n","\n","All inference is done on CPU using PyTorch and Hugging Face Transformers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTXEJleej-5q"},"outputs":[],"source":["!pip install transformers datasets torch fsspec==2023.6.0 psutil\n","\n","import torch.nn.utils.prune as prune\n","import copy\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","from datasets import load_dataset\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import time\n","import psutil\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"Vrd7S6VHnu7i"},"source":["## 1. Load Fine-Tuned Model and Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RA5EyK4pn0pK"},"outputs":[],"source":["model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","base_model = AutoModelForSequenceClassification.from_pretrained(model_id)"]},{"cell_type":"markdown","metadata":{"id":"RwHcAfPXn045"},"source":["## 2. Load and Preprocess SST-2 Validation Set"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"bznLjImvn1FM","executionInfo":{"status":"ok","timestamp":1753474466217,"user_tz":-180,"elapsed":2043,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"outputs":[],"source":["dataset = load_dataset(\"glue\", \"sst2\")\n","texts = dataset[\"validation\"][\"sentence\"][:]\n","labels = dataset[\"validation\"][\"label\"]\n","\n","encodings = tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")"]},{"cell_type":"markdown","metadata":{"id":"lhbieWHzn1SA"},"source":["## 3. Define Evaluation Dataset and DataLoader"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KGLvp907n1fj","executionInfo":{"status":"ok","timestamp":1753474468768,"user_tz":-180,"elapsed":21,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"outputs":[],"source":["class SST2Dataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.encodings[\"input_ids\"][idx],\n","            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n","            \"label\": torch.tensor(self.labels[idx]),\n","        }\n","\n","val_dataset = SST2Dataset(encodings, labels)\n","val_loader = DataLoader(val_dataset, batch_size=16)"]},{"cell_type":"markdown","metadata":{"id":"Q0ZYXI-in1ry"},"source":["## 4. Prune and Quantize the Model\n","\n","This function:\n","- Deepcopies the original fine-tuned model\n","- Applies **unstructured L1 pruning** to all `nn.Linear` layers\n","- Removes pruning masks (making pruning permanent)\n","- Applies **8-bit dynamic quantization** via PyTorch\n","\n","Returns the quantized pruned model."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"k6HCFct5n133","executionInfo":{"status":"ok","timestamp":1753474470600,"user_tz":-180,"elapsed":18,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"outputs":[],"source":["def prune_and_quantize(base_model, pruning_amount):\n","    model = copy.deepcopy(base_model)\n","\n","    # Apply pruning\n","    for name, module in model.named_modules():\n","        if isinstance(module, torch.nn.Linear):\n","            prune.l1_unstructured(module, name='weight', amount=pruning_amount)\n","\n","    # Remove pruning masks\n","    for name, module in model.named_modules():\n","        if isinstance(module, torch.nn.Linear):\n","            try:\n","                prune.remove(module, 'weight')\n","            except:\n","                pass\n","\n","    # Apply dynamic quantization\n","    model.cpu()\n","    quantized_model = torch.quantization.quantize_dynamic(\n","        model, {torch.nn.Linear}, dtype=torch.qint8\n","    )\n","\n","    return quantized_model"]},{"cell_type":"markdown","metadata":{"id":"Pp4Cuk0Pn2Cd"},"source":["## 5. Define Evaluation Function"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"7LQSVKmKn2L8","executionInfo":{"status":"ok","timestamp":1753474472593,"user_tz":-180,"elapsed":27,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"outputs":[],"source":["def evaluate_model(model, dataloader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    start = time.time()\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            inputs = {\n","                \"input_ids\": batch[\"input_ids\"],\n","                \"attention_mask\": batch[\"attention_mask\"]\n","            }\n","            outputs = model(**inputs)\n","            predictions = outputs.logits.argmax(dim=-1)\n","            correct += (predictions == batch[\"label\"]).sum().item()\n","            total += batch[\"label\"].size(0)\n","\n","    end = time.time()\n","    latency = end - start\n","    accuracy = correct / total * 100\n","    memory_mb = psutil.Process().memory_info().rss / (1024 * 1024)\n","\n","    return accuracy, latency, memory_mb\n"]},{"cell_type":"markdown","metadata":{"id":"FxP8mZX3n2Uk"},"source":["## 6. Run Compression Sweep\n","\n","We apply pruning at 30%, 40%, and 50%, followed by quantization and evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBWnIMBwn2dJ"},"outputs":[],"source":["results = []\n","for pruning_level in [0.3, 0.4, 0.5]:\n","    print(f\"Running test for pruning level: {int(pruning_level * 100)}%\")\n","    quantized_model = prune_and_quantize(base_model, pruning_level)\n","    acc, time_taken, mem = evaluate_model(quantized_model, val_loader)\n","    results.append({\n","        \"Pruning\": f\"{int(pruning_level * 100)}%\",\n","        \"Accuracy\": acc,\n","        \"Latency (s)\": time_taken,\n","        \"Memory (MB)\": mem\n","    })"]},{"cell_type":"code","source":["pd.DataFrame(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"8kPAilksPz3u","executionInfo":{"status":"ok","timestamp":1753474880671,"user_tz":-180,"elapsed":25,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}},"outputId":"b8f57032-91de-4a93-ecea-b26e3a494b9a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  Pruning   Accuracy  Latency (s)  Memory (MB)\n","0     30%  90.481651    60.211333  2330.031250\n","1     40%  88.532110    56.272866  2493.656250\n","2     50%  87.155963    47.522224  2697.054688"],"text/html":["\n","  <div id=\"df-47a23f3a-48ee-4a65-8bcb-7a7e664d31ec\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pruning</th>\n","      <th>Accuracy</th>\n","      <th>Latency (s)</th>\n","      <th>Memory (MB)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>30%</td>\n","      <td>90.481651</td>\n","      <td>60.211333</td>\n","      <td>2330.031250</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>40%</td>\n","      <td>88.532110</td>\n","      <td>56.272866</td>\n","      <td>2493.656250</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>50%</td>\n","      <td>87.155963</td>\n","      <td>47.522224</td>\n","      <td>2697.054688</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47a23f3a-48ee-4a65-8bcb-7a7e664d31ec')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-47a23f3a-48ee-4a65-8bcb-7a7e664d31ec button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-47a23f3a-48ee-4a65-8bcb-7a7e664d31ec');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-d85a6b67-992c-4fb5-90ba-1c0197e1ebbb\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d85a6b67-992c-4fb5-90ba-1c0197e1ebbb')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-d85a6b67-992c-4fb5-90ba-1c0197e1ebbb button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Pruning\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"30%\",\n          \"40%\",\n          \"50%\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.671062155534736,\n        \"min\": 87.1559633027523,\n        \"max\": 90.48165137614679,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          90.48165137614679,\n          88.53211009174312,\n          87.1559633027523\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latency (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.494853668853861,\n        \"min\": 47.52222394943237,\n        \"max\": 60.21133279800415,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          60.21133279800415,\n          56.27286648750305,\n          47.52222394943237\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Memory (MB)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 183.8705471596606,\n        \"min\": 2330.03125,\n        \"max\": 2697.0546875,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2330.03125,\n          2493.65625,\n          2697.0546875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"JRN1UDsOn2lP"},"source":["## 7. Summary\n","\n","Structured pruning combined with 8-bit dynamic quantization results in:\n","\n","- A **gradual drop in accuracy** as sparsity increases (expected)\n","- A **consistent drop in latency** with higher pruning (fewer ops)\n","- An **increase in RAM usage**, likely due to additional overhead from quantization structures or PyTorch internals\n","\n","These results confirm that pruning can offer latency improvements even on CPU, though memory usage does not drop unless a sparse-aware backend is used (e.g., ONNX Runtime or DeepSparse).\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFn4blXPPsrObqXjCI8WRz"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}