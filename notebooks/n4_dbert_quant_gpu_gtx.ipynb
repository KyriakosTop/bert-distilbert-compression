{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75664add",
   "metadata": {},
   "source": [
    "# Quantized Evaluation – DistilBERT on GTX 1050 Ti (FP32, 8-bit, 4-bit)\n",
    "\n",
    "This notebook benchmarks the inference performance of a fine-tuned [DistilBERT](https://huggingface.co/distilbert-base-uncased) model on the SST-2 sentiment classification task using a GTX 1050 Ti GPU.\n",
    "\n",
    "We evaluate the model in three configurations using Hugging Face Transformers and the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) quantization backend:\n",
    "\n",
    "- Full-precision (FP32) inference  \n",
    "- 8-bit quantized inference  \n",
    "- 4-bit quantized inference  \n",
    "\n",
    "Each model is tested on the first 100 samples from the SST-2 validation set.\n",
    "\n",
    "### Reported metrics:\n",
    "- Accuracy\n",
    "- Average latency per sample (in milliseconds)\n",
    "- GPU VRAM usage (after model load and during inference)\n",
    "- System RAM usage increase (in MB)\n",
    "\n",
    "All tests are executed using GPU only — but on a GTX 1050 Ti, which lacks support for 4-bit fused kernels and may trigger CPU fallback during quantized inference.\n",
    "\n",
    "**References:**  \n",
    "[1] Sanh et al., “DistilBERT: A distilled version of BERT,” https://arxiv.org/abs/1910.01108  \n",
    "[2] Dettmers et al., bitsandbytes library: https://github.com/TimDettmers/bitsandbytes  \n",
    "[3] GLUE Benchmark – SST-2: https://huggingface.co/datasets/glue/viewer/sst2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate bitsandbytes accelerate pynvml psutil\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import numpy as np\n",
    "import pynvml\n",
    "import psutil\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7890db",
   "metadata": {},
   "source": [
    "## 1. Load Pretrained DistilBERT and SST-2 Dataset\n",
    "\n",
    "We use the fine-tuned DistilBERT model for SST-2 from Hugging Face.\n",
    "\n",
    "The model is already trained and ready for inference.  \n",
    "We use the first 100 validation samples from SST-2 for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef260985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load SST-2 validation split (first 100 examples)\n",
    "raw_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:100]\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b047d1d",
   "metadata": {},
   "source": [
    "## 2. Convert to PyTorch-compatible dataset\n",
    "\n",
    "We define a custom dataset class that wraps the tokenized Hugging Face dataset and returns input tensors for PyTorch inference.\n",
    "\n",
    "All tensors are moved to GPU (`cuda:0`) during loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d764fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.input_ids = [torch.tensor(x).to(\"cuda\") for x in hf_dataset[\"input_ids\"]]\n",
    "        self.attention_mask = [torch.tensor(x).to(\"cuda\") for x in hf_dataset[\"attention_mask\"]]\n",
    "        self.labels = [torch.tensor(x) for x in hf_dataset[\"label\"]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"label\": self.labels[idx].item()\n",
    "        }\n",
    "\n",
    "dataset = SST2Dataset(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470ba13",
   "metadata": {},
   "source": [
    "## 3. Define Evaluation Function\n",
    "\n",
    "This function evaluates the model on the SST-2 validation set using GPU.\n",
    "\n",
    "It measures:\n",
    "- Accuracy\n",
    "- Average latency per sample (in seconds)\n",
    "- GPU VRAM usage after model load\n",
    "- GPU VRAM increase during inference\n",
    "- System RAM usage increase during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe0ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    process = psutil.Process(os.getpid())\n",
    "\n",
    "    # Initialize GPU memory tracker\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "    # Total VRAM used after model is loaded\n",
    "    model_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)  # MB\n",
    "\n",
    "    # RAM usage before inference\n",
    "    start_ram = process.memory_info().rss\n",
    "    start_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n",
    "\n",
    "    correct = 0\n",
    "    latencies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            inputs = {\n",
    "                \"input_ids\": sample[\"input_ids\"].unsqueeze(0),\n",
    "                \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0),\n",
    "            }\n",
    "            label = sample[\"label\"]\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(**inputs)\n",
    "            end_time = time.time()\n",
    "\n",
    "            pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "            correct += (pred == label)\n",
    "            latencies.append(end_time - start_time)\n",
    "\n",
    "    # Memory after inference\n",
    "    end_ram = process.memory_info().rss\n",
    "    end_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n",
    "    pynvml.nvmlShutdown()\n",
    "\n",
    "    # Metrics\n",
    "    delta_ram = (end_ram - start_ram) / (1024 ** 2)      # in MB\n",
    "    delta_vram = (end_vram - start_vram) / (1024 ** 2)   # in MB\n",
    "    avg_latency = np.mean(latencies)\n",
    "    accuracy = correct / len(dataset)\n",
    "\n",
    "    return accuracy, avg_latency, delta_ram, delta_vram, model_vram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe27877",
   "metadata": {},
   "source": [
    "## 4. Run Full-Precision (FP32) Inference on GPU\n",
    "\n",
    "We now evaluate the full-precision (FP32) DistilBERT model using the GTX 1050 Ti GPU.\n",
    "\n",
    "This serves as our performance baseline before applying any quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full-precision model to GPU\n",
    "model_fp32 = AutoModelForSequenceClassification.from_pretrained(model_id).to(\"cuda\")\n",
    "\n",
    "# Run evaluation\n",
    "accuracy_fp32, latency_fp32, ram_fp32, vram_delta_fp32, vram_model_fp32 = evaluate_model(model_fp32, dataset)\n",
    "\n",
    "print(f\"Accuracy (FP32): {accuracy_fp32:.2%}\")\n",
    "print(f\"Latency per sample (FP32): {latency_fp32:.4f} seconds\")\n",
    "print(f\"System RAM usage increase (FP32): {ram_fp32:.2f} MB\")\n",
    "print(f\"GPU VRAM usage increase during inference (FP32): {vram_delta_fp32:.2f} MB\")\n",
    "print(f\"Total VRAM after model load (FP32): {vram_model_fp32:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65260bc",
   "metadata": {},
   "source": [
    "## 5. Quantize the Model (8-bit)\n",
    "\n",
    "We apply 8-bit quantization to the full-precision DistilBERT model using `bitsandbytes`.\n",
    "\n",
    "This reduces the model's linear layers to 8-bit precision, keeping everything on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8939f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before loading new model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\": 0}  # Force all layers onto GPU 0\n",
    ")\n",
    "\n",
    "model_int8 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config_8bit\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d2894",
   "metadata": {},
   "source": [
    "## 6. Evaluate Quantized Model (8-bit)\n",
    "\n",
    "We now evaluate the 8-bit quantized DistilBERT model using the same procedure as in the FP32 baseline.\n",
    "\n",
    "If the model fell back to CPU, latency and RAM usage may reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13876036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (INT8): 94.00%\n",
      "Latency per sample (INT8): 0.0984 seconds\n",
      "System RAM usage increase (INT8): 48.71 MB\n",
      "GPU VRAM usage increase during inference (INT8): -12.38 MB\n",
      "Total VRAM after model load (INT8): 1574.64 MB\n"
     ]
    }
   ],
   "source": [
    "accuracy_int8, latency_int8, ram_int8, vram_delta_int8, vram_model_int8 = evaluate_model(model_int8, dataset)\n",
    "\n",
    "print(f\"Accuracy (INT8): {accuracy_int8:.2%}\")\n",
    "print(f\"Latency per sample (INT8): {latency_int8:.4f} seconds\")\n",
    "print(f\"System RAM usage increase (INT8): {ram_int8:.2f} MB\")\n",
    "print(f\"GPU VRAM usage increase during inference (INT8): {vram_delta_int8:.2f} MB\")\n",
    "print(f\"Total VRAM after model load (INT8): {vram_model_int8:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe774e",
   "metadata": {},
   "source": [
    "## 7. Attempt 4-bit Quantized Inference (Expected to Fail or Fallback)\n",
    "\n",
    "We attempt to load a 4-bit quantized version of DistilBERT using `bitsandbytes`.\n",
    "\n",
    "This typically fails on GPUs below compute capability 7.5 (e.g., GTX 1050 Ti), or falls back to CPU execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f412e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (4-bit): 93.00%\n",
      "Latency per sample (4-bit): 0.0113 seconds\n",
      "System RAM usage increase (4-bit): -4.93 MB\n",
      "GPU VRAM usage increase during inference (4-bit): -3.00 MB\n",
      "Total VRAM after model load (4-bit): 1715.14 MB\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU cache before loading 4-bit model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    bnb_config_4bit = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "\n",
    "    model_4bit = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config_4bit\n",
    "    )\n",
    "\n",
    "    acc_4bit, lat_4bit, ram_4bit, vram_delta_4bit, vram_model_4bit = evaluate_model(model_4bit, dataset)\n",
    "\n",
    "    print(f\"Accuracy (4-bit): {acc_4bit:.2%}\")\n",
    "    print(f\"Latency per sample (4-bit): {lat_4bit:.4f} seconds\")\n",
    "    print(f\"System RAM usage increase (4-bit): {ram_4bit:.2f} MB\")\n",
    "    print(f\"GPU VRAM usage increase during inference (4-bit): {vram_delta_4bit:.2f} MB\")\n",
    "    print(f\"Total VRAM after model load (4-bit): {vram_model_4bit:.2f} MB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"4-bit quantization failed or was not supported on this GPU.\")\n",
    "    print(\"Error message:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce2976",
   "metadata": {},
   "source": [
    "## 8. Summary: FP32 vs Quantized DistilBERT on GTX 1050 Ti\n",
    "\n",
    "The following table summarizes the results of all compression modes tested on the GTX 1050 Ti.\n",
    "\n",
    "Note: Both 8-bit and 4-bit quantized models loaded successfully, but likely fell back to CPU execution — as shown by increased latency and negative VRAM deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5405bbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>FP32</th>\n",
       "      <th>8-bit</th>\n",
       "      <th>4-bit</th>\n",
       "      <th>Observation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy (%)</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>Minor drop (4-bit)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Latency (ms)</td>\n",
       "      <td>9.153507</td>\n",
       "      <td>98.441241</td>\n",
       "      <td>11.274576</td>\n",
       "      <td>INT8/4-bit slower → CPU fallback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAM Increase (MB)</td>\n",
       "      <td>97.398438</td>\n",
       "      <td>48.707031</td>\n",
       "      <td>-4.933594</td>\n",
       "      <td>INT8 lower than FP32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VRAM Increase (MB)</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>-12.375000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>Negative VRAM deltas → fallback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total VRAM (MB)</td>\n",
       "      <td>1402.535156</td>\n",
       "      <td>1574.640625</td>\n",
       "      <td>1715.140625</td>\n",
       "      <td>VRAM usage increased with quantization</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Metric         FP32        8-bit        4-bit  \\\n",
       "0        Accuracy (%)    94.000000    94.000000    93.000000   \n",
       "1        Latency (ms)     9.153507    98.441241    11.274576   \n",
       "2   RAM Increase (MB)    97.398438    48.707031    -4.933594   \n",
       "3  VRAM Increase (MB)    20.125000   -12.375000    -3.000000   \n",
       "4     Total VRAM (MB)  1402.535156  1574.640625  1715.140625   \n",
       "\n",
       "                              Observation  \n",
       "0                      Minor drop (4-bit)  \n",
       "1        INT8/4-bit slower → CPU fallback  \n",
       "2                    INT8 lower than FP32  \n",
       "3         Negative VRAM deltas → fallback  \n",
       "4  VRAM usage increased with quantization  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"Accuracy (%)\",\n",
    "        \"Latency (ms)\",\n",
    "        \"RAM Increase (MB)\",\n",
    "        \"VRAM Increase (MB)\",\n",
    "        \"Total VRAM (MB)\"\n",
    "    ],\n",
    "    \"FP32\": [94.00, latency_fp32 * 1000, ram_fp32, vram_delta_fp32, vram_model_fp32],\n",
    "    \"8-bit\": [94.00, latency_int8 * 1000, ram_int8, vram_delta_int8, vram_model_int8],\n",
    "    \"4-bit\": [93.00, lat_4bit * 1000, ram_4bit, vram_delta_4bit, vram_model_4bit],\n",
    "    \"Observation\": [\n",
    "        \"Minor drop (4-bit)\",\n",
    "        \"INT8/4-bit slower → CPU fallback\",\n",
    "        \"INT8 lower than FP32\",\n",
    "        \"Negative VRAM deltas → fallback\",\n",
    "        \"VRAM usage increased with quantization\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f3340",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Quantization experiments on the GTX 1050 Ti revealed the following:\n",
    "\n",
    "- **8-bit and 4-bit quantized models successfully loaded**, but both likely fell back to **CPU execution** during inference. This is evidenced by:\n",
    "  - Increased latency compared to FP32\n",
    "  - Negative GPU VRAM deltas during inference\n",
    "  - No actual acceleration over FP32\n",
    "\n",
    "- **FP32 inference ran cleanly on the 1050 Ti**, with latency around 9.15 ms/sample and modest VRAM/RAM usage.\n",
    "\n",
    "- **Quantized model VRAM footprints were higher**, which is expected due to `bitsandbytes` internal structures, even if GPU execution is not fully utilized.\n",
    "\n",
    "These results highlight that **consumer GPUs like the GTX 1050 Ti are not well-suited for modern quantization workflows**, especially those requiring CUDA compute capability ≥7.5. While models load, true performance benefits are only seen on GPUs like the T4 or newer.\n",
    "\n",
    "This notebook serves as a realistic benchmark of what to expect from older hardware — and reinforces the importance of hardware-awareness in LLM deployment planning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
