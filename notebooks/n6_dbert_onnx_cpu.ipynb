{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2at1_2XiTyZN"
   },
   "source": [
    "# DistilBERT Inference on CPU with ONNX Runtime – FP32 and INT8 Quantization\n",
    "\n",
    "This notebook benchmarks the inference performance of the **DistilBERT** model fine-tuned on the **SST-2 sentiment classification task**, using **ONNX Runtime** in a **CPU-only** environment (Google Colab).\n",
    "\n",
    "We evaluate:\n",
    "- Full-precision (FP32) inference via ONNX  \n",
    "- 8-bit quantized inference using ONNX Runtime’s dynamic quantization tools [1]\n",
    "\n",
    "All evaluations are performed on the full SST-2 validation set [2].  \n",
    "We report:\n",
    "- **Accuracy**\n",
    "- **Total inference time** (in seconds)\n",
    "- **RAM usage increase** during evaluation (in MB)\n",
    "\n",
    "> ⚠️ Structured pruning is **not supported** in ONNX exports via PyTorch’s pruning API. See [Section 8](#8.-Limitations:-Structured-Pruning-and-ONNX) for details.\n",
    "\n",
    "**References:**  \n",
    "[1] ONNX Runtime Quantization Docs: https://onnxruntime.ai/docs/performance/quantization.html  \n",
    "[2] SST-2 from the GLUE Benchmark: https://huggingface.co/datasets/glue/viewer/sst2  \n",
    "[3] Sanh et al. (2019). *DistilBERT: A distilled version of BERT.* https://arxiv.org/abs/1910.01108\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 69689,
     "status": "ok",
     "timestamp": 1754237930941,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "6msLy6tGT0gD",
    "outputId": "73df9755-2c74-4d0d-ee74-0ccf0baba857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hTorch version: 2.6.0+cu124\n",
      "ONNX Runtime version: 1.22.1\n",
      "CPU: x86_64\n",
      "Total system RAM (GB): 12.67\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets onnx onnxruntime psutil\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import psutil\n",
    "import time\n",
    "import os\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import platform\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# System info\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"ONNX Runtime version:\", ort.__version__)\n",
    "print(\"CPU:\", platform.processor())\n",
    "print(\"Total system RAM (GB):\", round(psutil.virtual_memory().total / (1024**3), 2))\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DXqoeJTURwi"
   },
   "source": [
    "## 1. Load DistilBERT and SST-2 Validation Set\n",
    "\n",
    "We load the fine-tuned DistilBERT model for SST-2 from Hugging Face, along with the full validation split from the GLUE benchmark.  \n",
    "The dataset is tokenized to a fixed maximum length of 128 tokens and wrapped into a custom PyTorch-compatible dataset for ONNX evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523,
     "referenced_widgets": [
      "080c9a39ebb24cebad7bdb50c9f09a20",
      "bb5afb885a904d14a4324a752c4cb446",
      "cf95d9fd2b534412af84f2d861cc2861",
      "3c928865cab441fe9848b8cfebcb6d91",
      "74adf162a43c423ca5768234b63d12cf",
      "b6308bfdc6f145c18bf3b28ad4d8b642",
      "a59f5e6a97444cadba26e557bcefa93e",
      "6d8e90cdacc94ce7a3e87ef8054268bf",
      "11a93fd176044d058dda6399447de047",
      "f24a12e19b3849df94850bf02e47be7c",
      "c800bd4097b6443a9a4995bf1899d418",
      "132348f4db0b41bc84d9d33acfca6602",
      "b2b50fda763440a6aeae74f27e338b7b",
      "eee1ff66ee804d9d9a4d28d6b6b5bd31",
      "528cc06c1bb246f09fe92f4bfc3aa730",
      "35c336db34d9458da0624b0329a467b7",
      "8a1255a7d397429283014c94341772ff",
      "4acdafe577524701b7142fbf66a144e4",
      "b911c8480c694c29b43d79fd9dc037a5",
      "f62f466206184b42871e6cbce336684d",
      "73bae1e1b4ca43b1a3d38ab5334f1354",
      "9933f55a2e6445168d0300ef6276ddc9",
      "90f19dd44a8a4a81b9b7a6323269b02a",
      "edcf63b8b75d426a872f87485b41eac5",
      "c12dd1ea78a84b48b3ac8dda219de835",
      "e05a12ce46774a27ae318587b31ea9e0",
      "188038bcfb234a61925a5aba0a47e5bc",
      "929e5a6794734fb8904c3d06df934fe2",
      "aa1661a91b084410a8bac90430098824",
      "b398245e40e344328249cac8b66caa0d",
      "df90f0e1e4a042a8b15396c0b98406d4",
      "66d3f898b6b447f59b3cd67661eb4444",
      "ab2d9d78f0ab4e75a5ed87b0c7cf1a69",
      "696cf6c1e2a64c27b5035ee42686cfa6",
      "8b159950a10d4142992a53b4a6407a63",
      "dfcf9801cffc4a0cab09994394778e84",
      "a6597ad888d94aff8dd743d4c6097ee1",
      "9731ab7c67064757b38d8bf07f676162",
      "e22a79801c67431298428869bf580463",
      "8c9b81c5512a4742a07370fe28b67a20",
      "51796c2981d94eb496d37d1df7e39fb7",
      "3a908ab1e8bc4d3392e2edc5f9ad364c",
      "f2e2a0854f794911865196800101241d",
      "4002fb711fc8489b8f1d30779b21e613",
      "c6e2034bc4154f7cac94d9a2ff47c92a",
      "6bb27c6c67164d55b0bed7db52f14093",
      "7bb926d5514d436382d3cd5389df2043",
      "476fde29bff64b0885007d3f0a7f7480",
      "218192d06c8a40b1b768a3c46f2e9620",
      "dcda31a79a8f470fb37ac7f7a87252ea",
      "70eafd8cf9f948ebba198428c8b557c8",
      "8ba1506bff4045818439587321d70bcb",
      "d762b99e038344e09d1937671234df71",
      "32dd42d6129644c781a72e4867b42f03",
      "467aac6b34984d9e961100120875d13a",
      "e408ef8f9e33411b88ff43b294e738b6",
      "104a56db173c4ab3bfcd995e495ed7c9",
      "f22cd8d7eb3845daac68958e395b6869",
      "3b9a93e1451f4ddda7ac51fdeda12b8a",
      "d93a005f20c04e97bb6e7e7f8945e340",
      "21c9370351f34f249de922e3c1b7af10",
      "c4421df4fc934fc5a73fe0e4392e4ce5",
      "2b98c18c60234b4696b331618b682b11",
      "32422685106d47bab070fd2bdb07a2a1",
      "ed9bcd5d5b6345fd91c0a77b3439c007",
      "206b41c4ca604ac58ddfa0bab21f7bbb",
      "246152daae8347d0bb35b6c9f910d4a4",
      "1db2af3bdf774b05b47feedb17e42c80",
      "4e696874ec9e432eb3be1320a9df4c46",
      "69b99554eca94075a8797b8478405127",
      "5b36970fe8e749eb86d5c4bfff9df190",
      "29cd03ff546c4f0c8439f28b171555aa",
      "76b8835c2a6e4fc4a736ab85a62ced6f",
      "0273557c08ed47eeb8f60655efcc94fc",
      "1880022b2da04efeba42d88306e76d02",
      "9a9fe12e2f4048cca77e3ab1dac45f32",
      "cbfc9896e3b945aca4d4456628a953cd",
      "5f538e240d114f3c8ea0d1428d7ad62e",
      "2587dfb927134d5ba4538a9799e37f97",
      "b9e3110f60b94ad980397e64a392e7b2",
      "dd841140c3d74f5da1490cf465252d53",
      "94ef9eb507554a61b69eef3a5d451585",
      "757f955acef34de683f5a78fe32ffc49",
      "360e163b7c674bea916722aa29bba698",
      "c2409104abe84ba4a8e1f863b5c02dae",
      "794075b8ceeb498397f46e7fd05eb4ee",
      "1158cd69f0f44e91a575db1a48434d23",
      "bbec9eee8d8b42df8200ab00c569d89b",
      "91ccea2dc6254977964d60efcd0b0873",
      "2ebfd9942331414286e7b51e609464a4",
      "4d10b06bd3814995b7ed72368cd85a53",
      "2c68c9a085e044959ca24e30f58e6f69",
      "55dddb7724dc4a9797a33a3301154700",
      "7396c87ebc5b4d9c95ffd6e77b79a0d8",
      "7e87813abc474cf5b7ea07da37c0e2e4",
      "0f2595daca17444daf812a3fa49fa26b",
      "352cb24e34564e6ebc87258ed7f90f43",
      "95886d5c62804d75a7e1e458910996e5",
      "370b46aee3fa4cd3899ffb28643b85e9",
      "478066d13cfc4f2db72340ac18ab04c5",
      "324081e12e5348aabc2f7d22d77d3b48",
      "cc8db454c1414b68b20442f91ec52edb",
      "e453256106af4f099311017afaee234a",
      "7fe1ece91806474c9270e78adc81bf71",
      "69bad9fc49e44ab8b11c8f63b651c324",
      "a2530871a3f049459fa5750e38ab87db",
      "2ec86593df3945a7a6a6c3fae26bfbae",
      "a7b314d748db48c9b715ae67689159c4",
      "f83646327f7040e0bf9fa62e8c904dc4",
      "e8c0a6335346458bae50691d3cd40dae",
      "36850510abe749239bca381ce48ab7c5",
      "8f64d701866049669cf7ace4cf520733",
      "bef461a2b87d4cc1a825f3dada6ec261",
      "e61370a6565f4b768f41767bedf73b16",
      "178d95c5aa1d4ca5ab4bbd8a608da625",
      "4f43eace2cd44f6391e2c87821a63106",
      "dbb39f99e0f9420787b81581426fa6aa",
      "ed281741b2a849c38956f3e023812bfd",
      "03f7606eea6446ddad3052c7b550eb73",
      "117cad2fd5ea4381ac9736502eff5441",
      "df434ea909a54d04a8d0207af321725c",
      "b0a3db111f5b4addb0126cdd6d133ff6",
      "56b15189618e42408496e31b607a0064",
      "37446290b58649a08faa7ba427b216e9",
      "7000436a3d614cf19578561e4fae6806",
      "cb4dcdaef62f43f5ada670e5d83beda8",
      "6bc74363a6f240ecaa5c22a57c1a50da",
      "7128a6daf2da4042b082df4e2c4a45f2",
      "0bac5d9a151145d1a379052fd630358f",
      "1113e7a9851e4795b115a95c028bba5f",
      "604d37a19d7646d891b6c9f838cb95ac",
      "189f306966284fcab8ec179adb917104"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 19214,
     "status": "ok",
     "timestamp": 1754237950153,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "Bw0iUXh-UUhf",
    "outputId": "ce75107f-149f-418e-9e10-b379504ed9e8"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and fine-tuned model\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "# Load full SST-2 validation set\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function)\n",
    "\n",
    "# Wrap as a PyTorch-compatible Dataset\n",
    "class SST2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.input_ids = torch.tensor(hf_dataset[\"input_ids\"])\n",
    "        self.attention_mask = torch.tensor(hf_dataset[\"attention_mask\"])\n",
    "        self.labels = torch.tensor(hf_dataset[\"label\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "dataset = SST2Dataset(tokenized_dataset)\n",
    "print(f\"Loaded {len(dataset)} validation samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHZm_EcLUeR5"
   },
   "source": [
    "## 2. Export DistilBERT (FP32) to ONNX Format\n",
    "\n",
    "We export the full-precision PyTorch model to ONNX format using a representative input from the validation set.  \n",
    "The exported ONNX model will be used with ONNX Runtime for inference on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4849,
     "status": "ok",
     "timestamp": 1754237955034,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "v3sid_uhUebg",
    "outputId": "fa1dd9cf-7045-42e1-feea-7cbfff44c942"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported ONNX model to: distilbert_fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode and move to CPU\n",
    "model.eval().cpu()\n",
    "\n",
    "# Prepare dummy input using the first sample\n",
    "sample = dataset[0]\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample[\"input_ids\"].unsqueeze(0),\n",
    "    \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0)\n",
    "}\n",
    "\n",
    "# Export path\n",
    "onnx_fp32_path = \"distilbert_fp32.onnx\"\n",
    "\n",
    "# Export model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=(inputs_onnx[\"input_ids\"], inputs_onnx[\"attention_mask\"]),\n",
    "    f=onnx_fp32_path,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\"}, \"attention_mask\": {0: \"batch_size\"}},\n",
    "    opset_version=14 # Note: scaled_dot_product_attention requires opset >= 14\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Exported ONNX model to: {onnx_fp32_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0oPCW8nVT_g"
   },
   "source": [
    "## 3. Define ONNX Runtime Evaluation Function\n",
    "\n",
    "This function runs inference on the full SST-2 validation set using ONNX Runtime.  \n",
    "It computes classification accuracy, total inference time (in seconds), and peak RAM usage during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HujRk84QVYu4"
   },
   "outputs": [],
   "source": [
    "def evaluate_onnx_model(onnx_path, dataset):\n",
    "    # Initialize ONNX Runtime session\n",
    "    session = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "    # Get input names\n",
    "    input_names = {inp.name: inp.name for inp in session.get_inputs()}\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    # Memory and timing\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_ram = process.memory_info().rss\n",
    "    start_time = time.time()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        inputs = {\n",
    "            input_names[\"input_ids\"]: sample[\"input_ids\"].unsqueeze(0).numpy(),\n",
    "            input_names[\"attention_mask\"]: sample[\"attention_mask\"].unsqueeze(0).numpy()\n",
    "        }\n",
    "\n",
    "        outputs = session.run([output_name], inputs)[0]\n",
    "        pred = int(np.argmax(outputs))\n",
    "        label = int(sample[\"label\"])\n",
    "\n",
    "        correct += (pred == label)\n",
    "        total += 1\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_ram = process.memory_info().rss\n",
    "\n",
    "    accuracy = correct / total\n",
    "    total_latency = end_time - start_time\n",
    "    ram_usage_mb = (end_ram - start_ram) / (1024 ** 2)\n",
    "\n",
    "    return accuracy, total_latency, ram_usage_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "au4zuUMQV0hK"
   },
   "source": [
    "## 4. Evaluate FP32 ONNX Model on CPU\n",
    "\n",
    "We now run inference using the full-precision ONNX model on the full SST-2 validation set using ONNX Runtime.  \n",
    "This serves as our performance baseline for comparison with pruned and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192949,
     "status": "ok",
     "timestamp": 1754238148025,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "95zyf05KVlK0",
    "outputId": "4991a29f-95f4-4086-a03b-13b35d67a4f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (FP32 ONNX): 91.06%\n",
      "Total inference time (FP32 ONNX): 191.68 seconds\n",
      "RAM usage increase (FP32 ONNX): 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "accuracy_fp32, latency_fp32, ram_fp32 = evaluate_onnx_model(\"distilbert_fp32.onnx\", dataset)\n",
    "\n",
    "# Report results\n",
    "print(f\"Accuracy (FP32 ONNX): {accuracy_fp32:.2%}\")\n",
    "print(f\"Total inference time (FP32 ONNX): {latency_fp32:.2f} seconds\")\n",
    "print(f\"RAM usage increase (FP32 ONNX): {ram_fp32:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rS7K3U-Nt-ai"
   },
   "source": [
    "## 5. Quantize the ONNX Model to 8-bit (INT8)\n",
    "\n",
    "We use ONNX Runtime's post-training dynamic quantization tool to convert the full-precision model to 8-bit integers. This quantizes the model weights, reducing model size and potentially improving inference latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10467,
     "status": "ok",
     "timestamp": 1754238158489,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "lnUgOsblt-h7",
    "outputId": "93a10c8e-b34b-44d3-aebc-99b372ffbbe6"
   },
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "onnx_int8_path = \"distilbert_int8.onnx\"\n",
    "\n",
    "# Quantize model (weights-only dynamic quantization)\n",
    "quantize_dynamic(\n",
    "    model_input=\"distilbert_fp32.onnx\",\n",
    "    model_output=onnx_int8_path,\n",
    "    weight_type=QuantType.QInt8  # use QInt8 for signed int weights\n",
    ")\n",
    "\n",
    "print(f\"Quantized ONNX model saved to: {onnx_int8_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deNeVBFwvtfv"
   },
   "source": [
    "## 6. Evaluate the INT8 Quantized ONNX Model\n",
    "We evaluate the quantized model on the same SST-2 validation set using ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121364,
     "status": "ok",
     "timestamp": 1754238341694,
     "user": {
      "displayName": "Kyriakos T.",
      "userId": "13061897858849939057"
     },
     "user_tz": -180
    },
    "id": "-KOt0D3hvuuF",
    "outputId": "b152c243-648e-4232-fef8-e6839273a096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (INT8 ONNX): 90.48%\n",
      "Total inference time (INT8 ONNX): 121.08 seconds\n",
      "RAM usage increase (INT8 ONNX): 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "accuracy_int8, latency_int8, ram_int8 = evaluate_onnx_model(onnx_int8_path, dataset)\n",
    "\n",
    "# Report\n",
    "print(f\"Accuracy (INT8 ONNX): {accuracy_int8:.2%}\")\n",
    "print(f\"Total inference time (INT8 ONNX): {latency_int8:.2f} seconds\")\n",
    "print(f\"RAM usage increase (INT8 ONNX): {ram_int8:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7N3VPdeyLnh"
   },
   "source": [
    "## 7. Summary: ONNX FP32 vs INT8\n",
    "\n",
    "| Model     | Accuracy | Inference Time (s) | RAM Usage Increase (MB) |\n",
    "|-----------|----------|--------------------|--------------------------|\n",
    "| FP32 ONNX | 91.06%   | 191.68             | 0.00                     |\n",
    "| INT8 ONNX | 90.48%   | 121.08             | 0.00                     |\n",
    "\n",
    "The 8-bit quantized model achieves nearly identical classification accuracy to the original FP32 model, with a ~36.8% reduction in total inference time.\n",
    "\n",
    "**Notes:**\n",
    "- RAM usage measurements are based on Python process-level RSS deltas. Due to caching, memory reuse, or measurement granularity, the change may appear as 0.00 MB even when internal buffers are reallocated.\n",
    "- This benchmark uses dynamic (post-training) quantization. Only model weights are quantized to 8-bit integers; activations remain in floating-point. As a result, accuracy degradation is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFAFqOURyOW3"
   },
   "source": [
    "## 8. Limitations: Structured Pruning and ONNX\n",
    "\n",
    "This notebook focuses on benchmarking ONNX Runtime inference for DistilBERT models in two configurations: full-precision (FP32) and 8-bit dynamically quantized. Structured pruning, although used in our PyTorch CPU experiments, is not included here for the following reasons:\n",
    "\n",
    "- PyTorch’s `torch.nn.utils.prune` module does not modify layer structures. Instead, it applies binary masks over the original weights using reparameterization during the forward pass.\n",
    "- When such a pruned model is exported to ONNX, these masks are removed and the exported graph contains the original dense weight tensors — including any zeroed-out values.\n",
    "- ONNX Runtime executes inference on these dense weights, with no awareness of sparsity. As a result, pruning yields no performance benefit unless the model is manually restructured to remove the pruned dimensions entirely.\n",
    "\n",
    "These limitations are consistent with PyTorch’s official pruning documentation [1], and they explain why structured-pruned models were excluded from this ONNX evaluation. Instead, pruning results are presented in our PyTorch CPU-based benchmarking notebook (`n2_dbert_quant_prun_cpu.ipynb`).\n",
    "\n",
    "> [1] https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMiBhlLhvQzG4cW6PcK05j4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
