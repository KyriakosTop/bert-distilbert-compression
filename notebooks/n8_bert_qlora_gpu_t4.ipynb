{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-base on SST-2: FP32 vs QLoRA (4-bit)\n",
        "\n",
        "This notebook trains and evaluates the **BERT-base-uncased** model (~110M params) on the **SST-2** sentiment classification task using a **T4 GPU** in Google Colab.\n",
        "\n",
        "We compare two regimes using Hugging Face Transformers, **bitsandbytes**, and **PEFT/LoRA**:\n",
        "- **FP32 baseline:** full-precision fine-tuning  \n",
        "- **QLoRA (4-bit):** 4-bit quantized backbone + low-rank adapters (LoRA) trained\n",
        "\n",
        "To avoid Colab GPU limits, we use a time-capped setup by default:\n",
        "- Train subset: first **20,000** training examples\n",
        "- **2 epochs**\n",
        "- Max sequence length = 128\n",
        "\n",
        "### Reported metrics (for both models)\n",
        "- Trainable vs total parameters  \n",
        "- Training wall time (min) & peak VRAM (MB)  \n",
        "- **Full validation** (872 samples): accuracy, latency per sample (ms)  \n",
        "- System RAM Δ (MB), GPU VRAM Δ (MB), total VRAM after model load (MB)\n",
        "\n",
        "This notebook evaluates whether **QLoRA**, designed for **very large LLMs**, offers practical efficiency benefits at the **BERT** scale.\n",
        "\n",
        "**References:**  \n",
        "[1] Devlin, J. et al. (2019). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.* https://arxiv.org/abs/1810.04805  \n",
        "[2] Hu, E. et al. (2021). *LoRA: Low-Rank Adaptation of Large Language Models.* https://arxiv.org/abs/2106.09685  \n",
        "[3] Dettmers, T. et al. (2023). *QLoRA: Efficient Finetuning of Quantized LLMs.* https://arxiv.org/abs/2305.14314  \n",
        "[4] SST-2 (GLUE) dataset viewer: https://huggingface.co/datasets/glue/viewer/sst2  \n",
        "[5] bitsandbytes library: https://github.com/TimDettmers/bitsandbytes  \n",
        "[6] PEFT (Parameter-Efficient Fine-Tuning): https://github.com/huggingface/peft\n"
      ],
      "metadata": {
        "id": "e60_JTW4sGeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip -q install transformers datasets bitsandbytes accelerate peft psutil pynvml\n",
        "\n",
        "# Imports\n",
        "import os, time, sys, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        ")\n",
        "import transformers"
      ],
      "metadata": {
        "id": "rAXxOiCPsJlv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"Transformers:\", transformers.__version__)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA:\", torch.version.cuda, \"| GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X17mtvxhHRYr",
        "outputId": "c92d858d-bd37-4fb8-afb5-8141ee49f231"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11\n",
            "PyTorch: 2.8.0+cu126\n",
            "Transformers: 4.55.3\n",
            "CUDA: 12.6 | GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID   = \"bert-base-uncased\"\n",
        "TRAIN_SLICE = \"train[:20000]\"\n",
        "EPOCHS      = 2\n",
        "BATCH_TRAIN = 16\n",
        "BATCH_EVAL  = 32\n",
        "LR          = 2e-5\n",
        "MAX_LEN     = 128\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device info\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "Hdd9mOY3sSjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83933824-dc3a-465b-8297-ccf7c78859b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load & Tokenize SST-2\n",
        "\n",
        "We load:\n",
        "- **Train split (subset):** `train[:20,000]` (configurable via `TRAIN_SLICE`)\n",
        "- **Validation split (full):** 872 samples\n",
        "\n",
        "Tokenization uses `bert-base-uncased` with **max length = 128** to match the other notebooks."
      ],
      "metadata": {
        "id": "NUXysYFSs_Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Load SST-2 splits\n",
        "train_raw = load_dataset(\"glue\", \"sst2\", split=TRAIN_SLICE)\n",
        "val_raw   = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "# Tokenization function\n",
        "def tok_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"sentence\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "    )\n",
        "\n",
        "# Apply tokenization\n",
        "tok_train = train_raw.map(tok_fn, batched=True)\n",
        "tok_val   = val_raw.map(tok_fn, batched=True)\n",
        "\n",
        "# Clean columns for Trainer\n",
        "tok_train = tok_train.remove_columns([\"sentence\", \"idx\"]).rename_column(\"label\", \"labels\")\n",
        "tok_val   = tok_val.remove_columns([\"sentence\", \"idx\"]).rename_column(\"label\", \"labels\")\n",
        "\n",
        "print(f\"Train examples: {len(tok_train)} | Val examples: {len(tok_val)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7wy7Su9gtG7O",
        "outputId": "2d3134f5-ca5b-414d-817b-8df39e11a423"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train examples: 20000 | Val examples: 872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. FP32 Baseline Fine-Tuning\n",
        "\n",
        "We fine-tune **BERT-base-uncased** in FP32 on the SST-2 subset.\n",
        "\n",
        "**Hyperparameters:** batch size **16**, **epochs = 2**, **learning rate = 2e-5**, **max length = 128**.  \n",
        "We report **trainable vs total parameters**, **training wall time (min)**, and **peak GPU VRAM (MB)**.\n"
      ],
      "metadata": {
        "id": "NT0mXC8HtkUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Load FP32 model\n",
        "model_fp32 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_ID, num_labels=2\n",
        ").to(DEVICE)\n",
        "\n",
        "# Parameter counts\n",
        "trainable_params = sum(p.numel() for p in model_fp32.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model_fp32.parameters())\n",
        "print(f\"Trainable params: {trainable_params:,} | All params: {all_params:,} | Trainable%: {100*trainable_params/all_params:.2f}%\")\n",
        "\n",
        "# Training arguments (FP32)\n",
        "args_fp32 = TrainingArguments(\n",
        "    output_dir=\"n8_fp32_baseline\",\n",
        "    per_device_train_batch_size=BATCH_TRAIN,\n",
        "    per_device_eval_batch_size=BATCH_EVAL,\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        ")\n",
        "\n",
        "# Simple accuracy metric\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    return {\"accuracy\": float((preds == labels).mean())}\n",
        "\n",
        "# Trainer\n",
        "trainer_fp32 = Trainer(\n",
        "    model=model_fp32,\n",
        "    args=args_fp32,\n",
        "    train_dataset=tok_train,\n",
        "    eval_dataset=tok_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train and measure time/VRAM\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "t0 = time.time()\n",
        "_ = trainer_fp32.train()\n",
        "t1 = time.time()\n",
        "\n",
        "peak_vram_fp32 = (torch.cuda.max_memory_allocated() / (1024**2)) if torch.cuda.is_available() else 0.0\n",
        "wall_minutes_fp32 = (t1 - t0) / 60.0\n",
        "\n",
        "# Save the trained FP32 model (to ensure reproducibility if the runtime resets)\n",
        "os.makedirs(\"n8_fp32_baseline_ckpt\", exist_ok=True)\n",
        "model_fp32.save_pretrained(\"n8_fp32_baseline_ckpt\")\n",
        "tokenizer.save_pretrained(\"n8_fp32_baseline_ckpt\")\n",
        "\n",
        "print(f\" FP32 training complete | Wall time: {wall_minutes_fp32:.2f} min | Peak VRAM: {peak_vram_fp32:.2f} MB\")\n",
        "print(\" Saved checkpoint: n8_fp32_baseline_ckpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "collapsed": true,
        "id": "7gCRKr-otqZf",
        "outputId": "81d6531a-b4c5-4d4e-cce8-8bf7c07af0ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 109,483,778 | All params: 109,483,778 | Trainable%: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2740271944.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_fp32 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 14:33, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.233600</td>\n",
              "      <td>0.238257</td>\n",
              "      <td>0.902523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.138700</td>\n",
              "      <td>0.358879</td>\n",
              "      <td>0.916284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " FP32 training complete | Wall time: 14.59 min | Peak VRAM: 2526.11 MB\n",
            " Saved checkpoint: n8_fp32_baseline_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. QLoRA (4-bit) Fine-Tuning\n",
        "\n",
        "We fine-tune **BERT-base-uncased** with **QLoRA**: the backbone is loaded in **4-bit (nf4)** and we train **LoRA adapters**.\n",
        "\n",
        "> **Note:** QLoRA is a *different optimization regime* (tiny fraction of parameters are trainable), so we use **QLoRA-appropriate hyperparameters**:\n",
        "- LoRA rank **r = 16**, `target_modules=[\"query\",\"value\"]`, `lora_alpha=32`\n",
        "- **learning rate = 2e-4** (higher than FP32 to compensate for few trainable params)\n",
        "- **epochs = 2**, batch sizes as FP32\n",
        "- **4-bit compute dtype = fp16** (suited for T4)\n",
        "- **gradient checkpointing disabled** (for speed; uses a bit more VRAM)"
      ],
      "metadata": {
        "id": "QjVQFpIfwhBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4-bit quantization config (nf4)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,  # <- change from bfloat16 to float16 for T4\n",
        ")\n",
        "\n",
        "# Load model in 4-bit\n",
        "model_qlora = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    num_labels=2,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Prepare for k-bit training + attach LoRA adapters\n",
        "model_qlora = prepare_model_for_kbit_training(model_qlora)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=16,                 # <- was 8\n",
        "    lora_alpha=32,        # <- was 16\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\"],  # <- add this\n",
        "    bias=\"none\",\n",
        ")\n",
        "model_qlora = get_peft_model(model_qlora, lora_config)\n",
        "\n",
        "\n",
        "# Parameter counts\n",
        "trainable_params_qlora = sum(p.numel() for p in model_qlora.parameters() if p.requires_grad)\n",
        "all_params_qlora = sum(p.numel() for p in model_qlora.parameters())\n",
        "\n",
        "# Training arguments\n",
        "args_qlora = TrainingArguments(\n",
        "    output_dir=\"n8_qlora_output_r16\",\n",
        "    per_device_train_batch_size=BATCH_TRAIN,\n",
        "    per_device_eval_batch_size=BATCH_EVAL,\n",
        "    learning_rate=2e-4,           # <- higher LR for LoRA\n",
        "    num_train_epochs=EPOCHS,      # 2\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    gradient_checkpointing=False,  # <- speed up (uses a bit more VRAM)\n",
        ")\n",
        "\n",
        "# Reuse the same collator and compute_metrics as FP32\n",
        "trainer_qlora = Trainer(\n",
        "    model=model_qlora,\n",
        "    args=args_qlora,\n",
        "    train_dataset=tok_train,\n",
        "    eval_dataset=tok_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train and measure time/VRAM\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "t0 = time.time()\n",
        "_ = trainer_qlora.train()\n",
        "t1 = time.time()\n",
        "\n",
        "peak_vram_qlora = (torch.cuda.max_memory_allocated() / (1024**2)) if torch.cuda.is_available() else 0.0\n",
        "wall_minutes_qlora = (t1 - t0) / 60.0\n",
        "\n",
        "# Save QLoRA checkpoint\n",
        "os.makedirs(\"n8_qlora_ckpt\", exist_ok=True)\n",
        "model_qlora.save_pretrained(\"n8_qlora_ckpt\")\n",
        "tokenizer.save_pretrained(\"n8_qlora_ckpt\")\n",
        "\n",
        "print(f\" QLoRA training complete | Wall time: {wall_minutes_qlora:.2f} min | Peak VRAM: {peak_vram_qlora:.2f} MB\")\n",
        "print(\" Saved checkpoint: n8_qlora_ckpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "UlnPfQYgwlgc",
        "outputId": "88b1377f-8e27-45d3-a365-f3315e096c77"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 591,362 | All params: 67,312,900 | Trainable%: 0.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3470005274.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_qlora = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 07:30, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.286500</td>\n",
              "      <td>0.278059</td>\n",
              "      <td>0.893349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.237800</td>\n",
              "      <td>0.255994</td>\n",
              "      <td>0.905963</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " QLoRA training complete | Wall time: 7.55 min | Peak VRAM: 1638.87 MB\n",
            " Saved checkpoint: n8_qlora_ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QLoRA trainable % using both denominators\n",
        "# Assumes model_qlora (trained) and all_params (FP32 total) are in memory.\n",
        "trainable_params_qlora = sum(p.numel() for p in model_qlora.parameters() if p.requires_grad)\n",
        "all_params_qlora = sum(p.numel() for p in model_qlora.parameters())  # QLoRA tensor count\n",
        "\n",
        "pct_vs_qlora_total = 100 * trainable_params_qlora / all_params_qlora\n",
        "pct_vs_fp32_total  = 100 * trainable_params_qlora / all_params  # FP32 total from earlier\n",
        "\n",
        "print(\n",
        "    f\"Trainable params: {trainable_params_qlora:,} | \"\n",
        "    f\"All params (QLoRA tensors): {all_params_qlora:,} | \"\n",
        "    f\"Trainable% vs QLoRA total: {pct_vs_qlora_total:.2f}% | \"\n",
        "    f\"Trainable% vs FP32 arch total: {pct_vs_fp32_total:.2f}%\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAw3jvIRWD5X",
        "outputId": "ad48964c-d9a1-416d-f5b5-4730ba88dfad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 591,362 | All params (QLoRA tensors): 67,312,900 | Trainable% vs QLoRA total: 0.88% | Trainable% vs FP32 arch total: 0.54%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Evaluation (reload-based, per-model isolation)\n",
        "\n",
        "We reload each checkpoint and evaluate **one model at a time** to get correct:\n",
        "- Accuracy\n",
        "- Latency per sample (ms)\n",
        "- **VRAM after load (MB)** (only this model is on GPU)\n",
        "- **Peak VRAM during eval (MB)**"
      ],
      "metadata": {
        "id": "3ini8DKnQDae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MB = 1024**2\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Accurate per-model eval by reloading checkpoints (no other model on GPU)\n",
        "def _eval_loop(model, dataset, batch_size):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.eval().to(device)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
        "    n, correct = 0, 0\n",
        "    t0 = time.perf_counter()\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = model(**batch).logits\n",
        "            preds = logits.argmax(-1)\n",
        "            correct += (preds == batch[\"labels\"]).sum().item()\n",
        "            n += batch[\"labels\"].size(0)\n",
        "    t1 = time.perf_counter()\n",
        "    return (correct / n), (t1 - t0) * 1000.0 / n  # acc, ms/sample\n",
        "\n",
        "def eval_fp32_from_ckpt(ckpt_dir=\"n8_fp32_baseline_ckpt\"):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
        "        baseline = torch.cuda.memory_allocated()\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(ckpt_dir, num_labels=2)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    acc, ms = _eval_loop(model, tok_val, BATCH_EVAL)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        vram_after_load = (torch.cuda.memory_allocated() - baseline) / MB\n",
        "        peak_eval = torch.cuda.max_memory_allocated() / MB\n",
        "    else:\n",
        "        vram_after_load = peak_eval = 0.0\n",
        "    del model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    return {\n",
        "        \"accuracy\": round(acc, 4),\n",
        "        \"latency_ms_per_sample\": round(ms, 4),\n",
        "        \"vram_after_load_MB\": round(vram_after_load, 2),\n",
        "        \"peak_vram_eval_MB\": round(peak_eval, 2),\n",
        "    }\n",
        "\n",
        "def eval_qlora_from_ckpt(ckpt_dir=\"n8_qlora_ckpt\"):\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache(); torch.cuda.synchronize()\n",
        "        baseline = torch.cuda.memory_allocated()\n",
        "    # Reload 4-bit base then attach LoRA adapters\n",
        "    base = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_ID, num_labels=2, quantization_config=bnb_config, device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base, ckpt_dir)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    acc, ms = _eval_loop(model, tok_val, BATCH_EVAL)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        vram_after_load = (torch.cuda.memory_allocated() - baseline) / MB\n",
        "        peak_eval = torch.cuda.max_memory_allocated() / MB\n",
        "    else:\n",
        "        vram_after_load = peak_eval = 0.0\n",
        "    del model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    return {\n",
        "        \"accuracy\": round(acc, 4),\n",
        "        \"latency_ms_per_sample\": round(ms, 4),\n",
        "        \"vram_after_load_MB\": round(vram_after_load, 2),\n",
        "        \"peak_vram_eval_MB\": round(peak_eval, 2),\n",
        "    }\n",
        "\n",
        "# Run\n",
        "e_fp32 = eval_fp32_from_ckpt()\n",
        "e_qlor = eval_qlora_from_ckpt()\n",
        "e_fp32, e_qlor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFIbRXO3QDzW",
        "outputId": "c8cfe04b-7093-4ad4-f9f6-694347feea7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': 0.9163,\n",
              "  'latency_ms_per_sample': 7.5969,\n",
              "  'vram_after_load_MB': 420.99,\n",
              "  'peak_vram_eval_MB': 1513.67},\n",
              " {'accuracy': 0.906,\n",
              "  'latency_ms_per_sample': 3.2502,\n",
              "  'vram_after_load_MB': 99.0,\n",
              "  'peak_vram_eval_MB': 1124.68})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fair totals: use FP32 architecture total for both rows\n",
        "total_params_arch = all_params  # from FP32\n",
        "qlora_trainable_pct_arch = 100.0 * (trainable_params_qlora / total_params_arch)\n",
        "\n",
        "df_results = pd.DataFrame([\n",
        "    {\n",
        "        \"Model\": \"BERT FP32\",\n",
        "        \"Trainable / Total Params\": f\"{trainable_params:,} / {total_params_arch:,} (100%)\",\n",
        "        \"Training Wall Time (min)\": round(wall_minutes_fp32, 2),\n",
        "        \"Peak VRAM (MB, train)\": round(peak_vram_fp32, 2),\n",
        "        \"Val Acc\": e_fp32[\"accuracy\"],\n",
        "        \"Latency (ms/sample)\": e_fp32[\"latency_ms_per_sample\"],\n",
        "        \"VRAM after load (MB)\": e_fp32[\"vram_after_load_MB\"],\n",
        "        \"Peak VRAM (MB, eval)\": e_fp32[\"peak_vram_eval_MB\"],\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"BERT QLoRA (4-bit, r=16, q/v, fp16)\",\n",
        "        \"Trainable / Total Params\": f\"{trainable_params_qlora:,} / {total_params_arch:,} ({qlora_trainable_pct_arch:.2f}%)\",\n",
        "        \"Training Wall Time (min)\": round(wall_minutes_qlora, 2),\n",
        "        \"Peak VRAM (MB, train)\": round(peak_vram_qlora, 2),\n",
        "        \"Val Acc\": e_qlor[\"accuracy\"],\n",
        "        \"Latency (ms/sample)\": e_qlor[\"latency_ms_per_sample\"],\n",
        "        \"VRAM after load (MB)\": e_qlor[\"vram_after_load_MB\"],\n",
        "        \"Peak VRAM (MB, eval)\": e_qlor[\"peak_vram_eval_MB\"],\n",
        "    },\n",
        "])\n",
        "\n",
        "display(df_results)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "df_results.to_csv(\"results/bert_sst2_fp32_vs_qlora.csv\", index=False)\n",
        "print(\"Saved: results/bert_sst2_fp32_vs_qlora.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "_5zzoeLESJjv",
        "outputId": "05d4d9ac-8f13-4cd0-d904-f486b5eb587b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                 Model          Trainable / Total Params  \\\n",
              "0                            BERT FP32  109,483,778 / 109,483,778 (100%)   \n",
              "1  BERT QLoRA (4-bit, r=16, q/v, fp16)     591,362 / 109,483,778 (0.54%)   \n",
              "\n",
              "   Training Wall Time (min)  Peak VRAM (MB, train)  Val Acc  \\\n",
              "0                     14.59                2526.11   0.9163   \n",
              "1                      7.55                1638.87   0.9060   \n",
              "\n",
              "   Latency (ms/sample)  VRAM after load (MB)  Peak VRAM (MB, eval)  \n",
              "0               7.5969                420.99               1513.67  \n",
              "1               3.2502                 99.00               1124.68  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0930ca2a-9669-44d2-bf5d-a189c23ef421\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Trainable / Total Params</th>\n",
              "      <th>Training Wall Time (min)</th>\n",
              "      <th>Peak VRAM (MB, train)</th>\n",
              "      <th>Val Acc</th>\n",
              "      <th>Latency (ms/sample)</th>\n",
              "      <th>VRAM after load (MB)</th>\n",
              "      <th>Peak VRAM (MB, eval)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BERT FP32</td>\n",
              "      <td>109,483,778 / 109,483,778 (100%)</td>\n",
              "      <td>14.59</td>\n",
              "      <td>2526.11</td>\n",
              "      <td>0.9163</td>\n",
              "      <td>7.5969</td>\n",
              "      <td>420.99</td>\n",
              "      <td>1513.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>BERT QLoRA (4-bit, r=16, q/v, fp16)</td>\n",
              "      <td>591,362 / 109,483,778 (0.54%)</td>\n",
              "      <td>7.55</td>\n",
              "      <td>1638.87</td>\n",
              "      <td>0.9060</td>\n",
              "      <td>3.2502</td>\n",
              "      <td>99.00</td>\n",
              "      <td>1124.68</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0930ca2a-9669-44d2-bf5d-a189c23ef421')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0930ca2a-9669-44d2-bf5d-a189c23ef421 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0930ca2a-9669-44d2-bf5d-a189c23ef421');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-19b63b7a-690f-47c5-adfb-bb493b3f8c31\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-19b63b7a-690f-47c5-adfb-bb493b3f8c31')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-19b63b7a-690f-47c5-adfb-bb493b3f8c31 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_86190cd1-8719-4912-8a40-685d75d4bb42\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_86190cd1-8719-4912-8a40-685d75d4bb42 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"BERT QLoRA (4-bit, r=16, q/v, fp16)\",\n          \"BERT FP32\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Trainable / Total Params\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"591,362 / 109,483,778 (0.54%)\",\n          \"109,483,778 / 109,483,778 (100%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Training Wall Time (min)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.978031739553295,\n        \"min\": 7.55,\n        \"max\": 14.59,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          7.55,\n          14.59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Peak VRAM (MB, train)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 627.3734205399526,\n        \"min\": 1638.87,\n        \"max\": 2526.11,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1638.87,\n          2526.11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Val Acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0072831998462214225,\n        \"min\": 0.906,\n        \"max\": 0.9163,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.906,\n          0.9163\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Latency (ms/sample)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.073581045783566,\n        \"min\": 3.2502,\n        \"max\": 7.5969,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3.2502,\n          7.5969\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"VRAM after load (MB)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 227.68131247425643,\n        \"min\": 99.0,\n        \"max\": 420.99,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          99.0,\n          420.99\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Peak VRAM (MB, eval)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 275.0574668137551,\n        \"min\": 1124.68,\n        \"max\": 1513.67,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1124.68,\n          1513.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: results/bert_sst2_fp32_vs_qlora.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes.\n",
        "- FP32 total params used for both rows (QLoRA’s quantized layers don’t sum cleanly).\n",
        "- VRAM metrics come from per-model reload so each model is measured alone on GPU.\n",
        "- The “newly initialized classifier” warning can appear when loading the base model; it’s expected and does not affect results here."
      ],
      "metadata": {
        "id": "dd2JjJwsSOxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "**Setup:** SST-2 `train[:20,000]`, 2 epochs, max length 128, T4 GPU.\n",
        "\n",
        "| Model | Trainable / Total Params | Training Wall Time (min) | Peak VRAM (MB, train) | Val Acc | Latency (ms/sample) | VRAM after load (MB) | Peak VRAM (MB, eval) |\n",
        "|---|---|---:|---:|---:|---:|---:|---:|\n",
        "| BERT FP32 | 109,483,778 / 109,483,778 (100%) | 14.59 | 2526.11 | 0.9163 | 7.5969 | 420.99 | 1513.67 |\n",
        "| BERT QLoRA (4-bit, r=16, q/v, fp16) | 591,362 / 109,483,778 (0.54%) | 7.55 | 1638.87 | 0.9060 | 3.2502 | 99.00 | 1124.68 |\n",
        "\n",
        "**Notes.**\n",
        "- We report **FP32 total parameters** for both rows (QLoRA’s quantized layers don’t sum cleanly).\n",
        "- Evaluation memory metrics are from **per-model reload** so each model is measured alone on GPU.\n",
        "- The “newly initialized classifier” warning can appear when reloading the **base** model; it’s expected and does not affect results here.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "- **Accuracy:** QLoRA reaches **0.9060**, within **~1.0 point** of FP32 (**0.9163**).\n",
        "- **Training time:** QLoRA is ~**1.9× faster** (7.55 vs 14.59 min).\n",
        "- **Peak VRAM (train):** QLoRA saves ~**0.87 GB** (1638.87 vs 2526.11 MB).\n",
        "- **Inference:** QLoRA has ~**2.3× lower latency** (3.25 vs 7.60 ms/sample) and lower eval VRAM.\n",
        "\n",
        "**Takeaway:** At the BERT scale, **QLoRA** delivers strong **efficiency** (speed + memory) while achieving **near-FP32 accuracy**. FP32 remains the accuracy ceiling; QLoRA is ideal when VRAM or training time is constrained.\n"
      ],
      "metadata": {
        "id": "ny3XkjueSQBZ"
      }
    }
  ]
}