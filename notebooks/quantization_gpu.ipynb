{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOO2R/3hRve6R9oICjeHbHd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Quantized Evaluation – DistilBERT on GPU (FP32, 8-bit, 4-bit)\n","\n","This notebook benchmarks the inference performance of a fine-tuned DistilBERT model on the SST-2 sentiment classification task using a T4 GPU in Google Colab.\n","\n","We compare:\n","\n","- Full-precision (FP32) inference  \n","- 8-bit quantized inference using bitsandbytes  \n","- 4-bit quantized inference using bitsandbytes  \n","\n","Each version is evaluated on a subset of 100 samples from the SST-2 validation set.\n","\n","### Reported metrics:\n","- Accuracy\n","- Average latency per sample (in milliseconds)\n","- GPU memory usage (total VRAM after model load, and increase during inference)\n","- System RAM usage during inference\n","\n","All experiments use Hugging Face Transformers and bitsandbytes, executed in a GPU-only setup without CPU fallback."],"metadata":{"id":"TNxJm6h4kotv"}},{"cell_type":"code","source":["!pip install -q transformers datasets evaluate bitsandbytes accelerate\n","\n","import torch\n","from torch.utils.data import Dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n","from datasets import load_dataset\n","import time\n","import numpy as np\n","import pynvml\n","import psutil\n","import os"],"metadata":{"id":"5jpL1TgXkqZY","executionInfo":{"status":"ok","timestamp":1753465559015,"user_tz":-180,"elapsed":19757,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## 1. Load Pretrained DistilBERT and SST-2 Dataset\n","\n","We use the fine-tuned DistilBERT model for SST-2 from Hugging Face.\n","\n","The model is already trained and ready for inference.  \n","We use the first 100 validation samples from SST-2 for benchmarking."],"metadata":{"id":"fizak65ak6Qq"}},{"cell_type":"code","source":["model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# Load SST-2 validation split (first 100 examples)\n","raw_dataset = load_dataset(\"glue\", \"sst2\", split=\"validation[:100]\")\n","\n","# Tokenize the dataset\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n","\n","tokenized_dataset = raw_dataset.map(tokenize_function)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pNQSLTBgk8cJ","executionInfo":{"status":"ok","timestamp":1753465563016,"user_tz":-180,"elapsed":4017,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}},"outputId":"396c87f3-0c5e-4a63-c80b-45185d009701"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## 2. Convert to PyTorch-compatible dataset\n","\n","We define a custom dataset class that wraps the tokenized Hugging Face dataset and returns input tensors for PyTorch inference."],"metadata":{"id":"ROo8R0I6m7fq"}},{"cell_type":"code","source":["class SST2Dataset(Dataset):\n","    def __init__(self, hf_dataset):\n","        self.input_ids = [torch.tensor(x) for x in hf_dataset[\"input_ids\"]]\n","        self.attention_mask = [torch.tensor(x) for x in hf_dataset[\"attention_mask\"]]\n","        self.labels = [torch.tensor(x) for x in hf_dataset[\"label\"]]\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.input_ids[idx].to(\"cuda\"),\n","            \"attention_mask\": self.attention_mask[idx].to(\"cuda\"),\n","            \"label\": self.labels[idx].item()\n","        }\n","\n","dataset = SST2Dataset(tokenized_dataset)"],"metadata":{"id":"VAsgUGjIm7oo","executionInfo":{"status":"ok","timestamp":1753465563094,"user_tz":-180,"elapsed":81,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## 3. Define evaluation function\n","\n","This function evaluates the model on the SST-2 validation set using GPU.\n","\n","It measures:\n","\n","- Accuracy (on 100 SST-2 samples)\n","- Average latency per sample (in seconds)\n","- GPU VRAM used after model load (total footprint)\n","- GPU VRAM increase during inference (in MB)\n","- System RAM usage increase during inference (in MB)"],"metadata":{"id":"RTYiC5wSm77K"}},{"cell_type":"code","source":["def evaluate_model(model, dataset):\n","    model.eval()\n","    process = psutil.Process(os.getpid())\n","\n","    # Initialize GPU memory tracker\n","    pynvml.nvmlInit()\n","    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n","\n","    # Total VRAM used after model is loaded\n","    model_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 ** 2)  # MB\n","\n","    # RAM usage before inference\n","    start_ram = process.memory_info().rss\n","    start_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n","\n","    correct = 0\n","    latencies = []\n","\n","    with torch.no_grad():\n","        for sample in dataset:\n","            inputs = {\n","                \"input_ids\": sample[\"input_ids\"].unsqueeze(0),\n","                \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0),\n","            }\n","            label = sample[\"label\"]\n","\n","            start_time = time.time()\n","            outputs = model(**inputs)\n","            end_time = time.time()\n","\n","            pred = torch.argmax(outputs.logits, dim=1).item()\n","            correct += (pred == label)\n","            latencies.append(end_time - start_time)\n","\n","    # Memory after inference\n","    end_ram = process.memory_info().rss\n","    end_vram = pynvml.nvmlDeviceGetMemoryInfo(handle).used\n","    pynvml.nvmlShutdown()\n","\n","    # Metrics\n","    delta_ram = (end_ram - start_ram) / (1024 ** 2)      # in MB\n","    delta_vram = (end_vram - start_vram) / (1024 ** 2)   # in MB\n","    avg_latency = np.mean(latencies)\n","    accuracy = correct / len(dataset)\n","\n","    return accuracy, avg_latency, delta_ram, delta_vram, model_vram"],"metadata":{"id":"4XNzML2xm8CY","executionInfo":{"status":"ok","timestamp":1753465563150,"user_tz":-180,"elapsed":32,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## 4. Run Full-Precision (FP32) Evaluation on GPU\n","\n","We now evaluate the full-precision (FP32) DistilBERT model using the T4 GPU.\n","\n","This serves as our performance baseline before applying any quantization."],"metadata":{"id":"lqu8uN3lnhAW"}},{"cell_type":"code","source":["# Load full-precision model to GPU\n","model_fp32 = AutoModelForSequenceClassification.from_pretrained(model_id).to(\"cuda\")\n","\n","# Run evaluation\n","accuracy_fp32, latency_fp32, ram_fp32, vram_delta_fp32, vram_model_fp32 = evaluate_model(model_fp32, dataset)\n","\n","print(f\"Accuracy (FP32): {accuracy_fp32:.2%}\")\n","print(f\"Latency per sample (FP32): {latency_fp32:.4f} seconds\")\n","print(f\"System RAM usage increase (FP32): {ram_fp32:.2f} MB\")\n","print(f\"GPU VRAM usage increase during inference (FP32): {vram_delta_fp32:.2f} MB\")\n","print(f\"Total VRAM after model load (FP32): {vram_model_fp32:.2f} MB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHeWTMd-niHj","executionInfo":{"status":"ok","timestamp":1753465571087,"user_tz":-180,"elapsed":7940,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}},"outputId":"e7bba567-6c31-4a3d-afb0-3f13c5d53c1d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (FP32): 94.00%\n","Latency per sample (FP32): 0.0124 seconds\n","System RAM usage increase (FP32): 213.52 MB\n","GPU VRAM usage increase during inference (FP32): 30.00 MB\n","Total VRAM after model load (FP32): 659.88 MB\n"]}]},{"cell_type":"markdown","source":["## 5. Quantize the Model (8-bit)\n","\n","We apply 8-bit quantization to the full-precision DistilBERT model using bitsandbytes.\n","\n","This reduces the size of the model by compressing the linear layers to 8-bit precision. The quantized model remains compatible with GPU inference through Hugging Face Transformers."],"metadata":{"id":"kRdXXTiOqbB_"}},{"cell_type":"code","source":["# Clear GPU cache before loading new model\n","torch.cuda.empty_cache()\n","\n","# Load 8-bit quantized model fully on GPU\n","bnb_config_8bit = BitsAndBytesConfig(\n","    load_in_8bit=True,\n","    device_map={\"\": 0}  # Force all layers onto GPU 0\n",")\n","\n","model_int8 = AutoModelForSequenceClassification.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config_8bit\n",")"],"metadata":{"id":"6OQYvtmEqb1s","executionInfo":{"status":"ok","timestamp":1753465572012,"user_tz":-180,"elapsed":928,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## 6. Evaluate Quantized Model (INT8)\n","\n","We evaluate the 8-bit quantized DistilBERT model using the same procedure as in the FP32 baseline.\n"],"metadata":{"id":"0JKJzGOeq7qx"}},{"cell_type":"code","source":["accuracy_int8, latency_int8, ram_int8, vram_delta_int8, vram_model_int8 = evaluate_model(model_int8, dataset)\n","\n","print(f\"Accuracy (INT8): {accuracy_int8:.2%}\")\n","print(f\"Latency per sample (INT8): {latency_int8:.4f} seconds\")\n","print(f\"System RAM usage increase (INT8): {ram_int8:.2f} MB\")\n","print(f\"GPU VRAM usage increase during inference (INT8): {vram_delta_int8:.2f} MB\")\n","print(f\"Total VRAM after model load (INT8): {vram_model_int8:.2f} MB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_DhKCgNrA_C","executionInfo":{"status":"ok","timestamp":1753465579777,"user_tz":-180,"elapsed":7768,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}},"outputId":"03bcba8a-af69-46d6-f7ee-a69179f4efdc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (INT8): 94.00%\n","Latency per sample (INT8): 0.0772 seconds\n","System RAM usage increase (INT8): 130.85 MB\n","GPU VRAM usage increase during inference (INT8): 14.00 MB\n","Total VRAM after model load (INT8): 843.88 MB\n"]}]},{"cell_type":"markdown","source":["## 7. Quantize the Model (4-bit)\n","\n","We apply 4-bit quantization using bitsandbytes by enabling `load_in_4bit=True`.\n","\n","This uses QLoRA-style 4-bit quantization with `bnb_4bit` kernels, compressing linear layers more aggressively than 8-bit. We expect a further reduction in memory footprint, potentially at the cost of slightly slower inference or reduced accuracy."],"metadata":{"id":"9AfiQscuvJt_"}},{"cell_type":"code","source":["# Clear GPU cache before loading 4-bit model\n","torch.cuda.empty_cache()\n","\n","bnb_config_4bit = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    device_map={\"\": 0}\n",")\n","\n","model_4bit = AutoModelForSequenceClassification.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config_4bit\n",")"],"metadata":{"id":"wzrsDBtgvK7j","executionInfo":{"status":"ok","timestamp":1753466322482,"user_tz":-180,"elapsed":600,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## 8. Evaluate Quantized Model (4-bit)\n","\n","We evaluate the 4-bit quantized DistilBERT model using the same procedure.\n","\n","The 4-bit setup is more aggressive than 8-bit and is typically used for large models with QLoRA. On small models like DistilBERT, results may vary."],"metadata":{"id":"csch7IXjvQMX"}},{"cell_type":"code","source":["accuracy_4bit, latency_4bit, ram_4bit, vram_delta_4bit, vram_model_4bit = evaluate_model(model_4bit, dataset)\n","\n","print(f\"Accuracy (4-bit): {accuracy_4bit:.2%}\")\n","print(f\"Latency per sample (4-bit): {latency_4bit:.4f} seconds\")\n","print(f\"System RAM usage increase (4-bit): {ram_4bit:.2f} MB\")\n","print(f\"GPU VRAM usage increase during inference (4-bit): {vram_delta_4bit:.2f} MB\")\n","print(f\"Total VRAM after model load (4-bit): {vram_model_4bit:.2f} MB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DMcpOo00vQ3u","executionInfo":{"status":"ok","timestamp":1753466349378,"user_tz":-180,"elapsed":1684,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}},"outputId":"5ac518c1-7e81-46fd-bf71-d345e19b4fa6"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (4-bit): 93.00%\n","Latency per sample (4-bit): 0.0165 seconds\n","System RAM usage increase (4-bit): 5.73 MB\n","GPU VRAM usage increase during inference (4-bit): 2.00 MB\n","Total VRAM after model load (4-bit): 953.88 MB\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"pTvCcYMzwlxY"}},{"cell_type":"markdown","source":["## 9. Summary: FP32 vs Quantized DistilBERT on GPU (T4)\n","\n","The following table summarizes the results from the experimentations."],"metadata":{"id":"nYaXiWwZylUY"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.DataFrame({\n","    \"Metric\": [\n","        \"Accuracy (%)\",\n","        \"Latency (ms)\",\n","        \"RAM Increase (MB)\",\n","        \"VRAM Increase (MB)\",\n","        \"Total VRAM (MB)\"\n","    ],\n","    \"FP32\": [94.00, 12.40, 213.52, 30.00, 659.88],\n","    \"8-bit\": [94.00, 77.20, 130.85, 14.00, 843.88],\n","    \"4-bit\": [93.00, 16.50, 5.73, 2.00, 953.88],\n","    \"Change (vs FP32)\": [\n","        \"-1.0% (4-bit)\",\n","        \"8-bit slower\",\n","        \"Lower with quantized\",\n","        \"Lower with quantized\",\n","        \"Higher in all cases\"\n","    ]\n","})\n","\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"bO8Ogj93ypEK","executionInfo":{"status":"ok","timestamp":1753467235199,"user_tz":-180,"elapsed":60,"user":{"displayName":"Kyriakos T.","userId":"13061897858849939057"}},"outputId":"9d9f47e1-0da3-4ff8-c7b7-8345bd53fb93"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["               Metric    FP32   8-bit   4-bit      Change (vs FP32)\n","0        Accuracy (%)   94.00   94.00   93.00         -1.0% (4-bit)\n","1        Latency (ms)   12.40   77.20   16.50          8-bit slower\n","2   RAM Increase (MB)  213.52  130.85    5.73  Lower with quantized\n","3  VRAM Increase (MB)   30.00   14.00    2.00  Lower with quantized\n","4     Total VRAM (MB)  659.88  843.88  953.88   Higher in all cases"],"text/html":["\n","  <div id=\"df-3cfaceef-5439-438d-896c-dc981a1878c7\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Metric</th>\n","      <th>FP32</th>\n","      <th>8-bit</th>\n","      <th>4-bit</th>\n","      <th>Change (vs FP32)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Accuracy (%)</td>\n","      <td>94.00</td>\n","      <td>94.00</td>\n","      <td>93.00</td>\n","      <td>-1.0% (4-bit)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Latency (ms)</td>\n","      <td>12.40</td>\n","      <td>77.20</td>\n","      <td>16.50</td>\n","      <td>8-bit slower</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>RAM Increase (MB)</td>\n","      <td>213.52</td>\n","      <td>130.85</td>\n","      <td>5.73</td>\n","      <td>Lower with quantized</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>VRAM Increase (MB)</td>\n","      <td>30.00</td>\n","      <td>14.00</td>\n","      <td>2.00</td>\n","      <td>Lower with quantized</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Total VRAM (MB)</td>\n","      <td>659.88</td>\n","      <td>843.88</td>\n","      <td>953.88</td>\n","      <td>Higher in all cases</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3cfaceef-5439-438d-896c-dc981a1878c7')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3cfaceef-5439-438d-896c-dc981a1878c7 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3cfaceef-5439-438d-896c-dc981a1878c7');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-4e361323-7000-4f3a-b2d8-31aa4f24b6d2\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e361323-7000-4f3a-b2d8-31aa4f24b6d2')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-4e361323-7000-4f3a-b2d8-31aa4f24b6d2 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_d7a56638-998e-4ab6-923f-3791dc366a0f\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_d7a56638-998e-4ab6-923f-3791dc366a0f button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Latency (ms)\",\n          \"Total VRAM (MB)\",\n          \"RAM Increase (MB)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FP32\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 267.85395124955687,\n        \"min\": 12.4,\n        \"max\": 659.88,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          12.4,\n          659.88,\n          213.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"8-bit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 344.65906919737364,\n        \"min\": 14.0,\n        \"max\": 843.88,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          77.2,\n          843.88,\n          130.85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4-bit\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 415.147507182688,\n        \"min\": 2.0,\n        \"max\": 953.88,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          16.5,\n          953.88,\n          5.73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Change (vs FP32)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"8-bit slower\",\n          \"Higher in all cases\",\n          \"-1.0% (4-bit)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["**Observations:**\n","\n","- Despite reducing inference memory deltas, bitsandbytes **increased total VRAM usage** on GPU for DistilBERT.\n","- Latency increased in 8-bit mode – likely due to overhead from fused CUDA kernels and submodule offloading.\n","- Accuracy remained stable across all configurations.\n","- These findings suggest that **bitsandbytes quantization is not beneficial for small models like DistilBERT** on GPU, despite being effective on CPU.\n","- For meaningful GPU compression gains, 4-bit/8-bit quantization should be evaluated on **larger models** (e.g., BERT, LLaMA) where benefits outweigh overhead."],"metadata":{"id":"F80XZwCJys0F"}}]}